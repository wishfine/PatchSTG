"""
ODPS é¢„å¤„ç†æ•°æ®åŠ è½½å™¨
ä½¿ç”¨é¢„å¤„ç†åçš„æ—¶é—´åºåˆ—è¡¨ï¼štb_inter_traffic_timeseries

æ•°æ®æ ¼å¼ï¼š
- æ¯æ¡è®°å½• = ä¸€ä¸ªæ—¶é—´çª—å£çš„æ‰€æœ‰èŠ‚ç‚¹æ•°æ®
- flow_matrix_sparse: "0:15;1:8;2:0;..." (èŠ‚ç‚¹ç´¢å¼•:æµé‡å€¼)
- history_matrix_sparse: "0:5;3;2;...;1:7;8;..." (èŠ‚ç‚¹ç´¢å¼•:å†å²åºåˆ—)
"""

import os
import numpy as np
from torch.utils.data import Dataset, DataLoader
from typing import Dict, List, Tuple
from lib.utils import log_string


class PreprocessedODPSDataset(Dataset):
    """é¢„å¤„ç†åçš„ ODPS æ•°æ®é›†"""
    
    def __init__(self, 
                 odps_instance,
                 table_name: str,
                 meta_table_name: str,
                 adcode: str,
                 start_date: str,
                 end_date: str,
                 input_len: int = 12,
                 output_len: int = 12,
                 log=None):
        """
        Args:
            odps_instance: ODPS è¿æ¥å®ä¾‹
            table_name: é¢„å¤„ç†åçš„æ—¶é—´åºåˆ—è¡¨åï¼ˆtb_inter_traffic_timeseriesï¼‰
            meta_table_name: èŠ‚ç‚¹å…ƒæ•°æ®è¡¨åï¼ˆtb_inter_node_metadataï¼‰
            adcode: åŸå¸‚ä»£ç 
            start_date: å¼€å§‹æ—¥æœŸ 'YYYYMMDD'
            end_date: ç»“æŸæ—¥æœŸ 'YYYYMMDD'
            input_len: è¾“å…¥åºåˆ—é•¿åº¦
            output_len: è¾“å‡ºåºåˆ—é•¿åº¦
            log: æ—¥å¿—å‡½æ•°
        """
        self.odps = odps_instance
        self.table_name = table_name
        self.meta_table_name = meta_table_name
        self.adcode = adcode
        self.start_date = start_date
        self.end_date = end_date
        self.input_len = input_len
        self.output_len = output_len
        self.log = log or print
        
        # åŠ è½½èŠ‚ç‚¹å…ƒæ•°æ®
        self._load_node_metadata()
        
        # åŠ è½½æ—¶é—´åºåˆ—æ•°æ®
        self._load_timeseries_data()
        
        log_string(self.log, f"âœ… æ•°æ®é›†åˆå§‹åŒ–å®Œæˆ:")
        log_string(self.log, f"   - èŠ‚ç‚¹æ•°: {self.num_nodes}")
        log_string(self.log, f"   - æ—¶é—´çª—å£æ•°: {len(self.time_windows)}")
        log_string(self.log, f"   - æ ·æœ¬æ•°: {len(self)}")
        log_string(self.log, f"   - è¾“å…¥é•¿åº¦: {self.input_len}, è¾“å‡ºé•¿åº¦: {self.output_len}")
        
    def _load_node_metadata(self):
        """åŠ è½½èŠ‚ç‚¹å…ƒæ•°æ®"""
        log_string(self.log, f"ğŸ“– åŠ è½½èŠ‚ç‚¹å…ƒæ•°æ®: {self.meta_table_name}")
        
        query = f"""
        SELECT 
            node_idx,
            nds_id,
            next_nds_id,
            inter_id,
            lat,
            lng
        FROM {self.meta_table_name}
        ORDER BY node_idx
        """
        
        self.node_metadata = []
        self.node_locations = []
        
        with self.odps.execute_sql(query).open_reader() as reader:
            for record in reader:
                node_idx = record['node_idx']
                lat = record['lat']
                lng = record['lng']
                
                self.node_metadata.append({
                    'node_idx': node_idx,
                    'nds_id': record['nds_id'],
                    'next_nds_id': record['next_nds_id'],
                    'inter_id': record['inter_id']
                })
                
                # ä½ç½®ä¿¡æ¯ï¼ˆç”¨äºè®¡ç®—è·ç¦»çŸ©é˜µï¼‰
                if lat is not None and lng is not None:
                    self.node_locations.append([lat, lng])
                else:
                    self.node_locations.append([0.0, 0.0])  # é»˜è®¤å€¼
        
        self.num_nodes = len(self.node_metadata)
        self.node_locations = np.array(self.node_locations)
        
        # ç»Ÿè®¡ä½ç½®è¦†ç›–ç‡
        valid_locations = np.sum((self.node_locations != 0).any(axis=1))
        coverage = valid_locations / self.num_nodes * 100
        
        log_string(self.log, f"   âœ… èŠ‚ç‚¹å…ƒæ•°æ®åŠ è½½å®Œæˆ: {self.num_nodes} ä¸ªèŠ‚ç‚¹")
        log_string(self.log, f"   ğŸ“ ä½ç½®è¦†ç›–ç‡: {coverage:.1f}%")
        
        assert self.num_nodes > 0, "èŠ‚ç‚¹æ•°é‡ä¸º0"
        
    def _load_timeseries_data(self):
        """åŠ è½½æ—¶é—´åºåˆ—æ•°æ®"""
        log_string(self.log, f"ğŸ“– åŠ è½½æ—¶é—´åºåˆ—æ•°æ®: {self.table_name}")
        log_string(self.log, f"   - åŸå¸‚: {self.adcode}")
        log_string(self.log, f"   - æ—¥æœŸèŒƒå›´: {self.start_date} ~ {self.end_date}")
        
        query = f"""
        SELECT 
            time_window,
            time_features,
            node_count,
            flow_matrix_sparse,
            history_matrix_sparse
        FROM {self.table_name}
        WHERE adcode = '{self.adcode}'
          AND ds >= '{self.start_date}'
          AND ds <= '{self.end_date}'
        ORDER BY time_window
        """
        
        self.time_windows = []
        self.flow_matrices = []
        self.history_matrices = []
        self.time_features = []
        
        with self.odps.execute_sql(query).open_reader() as reader:
            for record in reader:
                time_window = record['time_window']
                node_count = record['node_count']
                
                # è§£æç¨€ç–æµé‡çŸ©é˜µ
                flow_dict = {}
                if record['flow_matrix_sparse']:
                    for item in record['flow_matrix_sparse'].split(';'):
                        if ':' in item:
                            idx, flow = item.split(':')
                            flow_dict[int(idx)] = float(flow)
                
                # æ„å»ºå¯†é›†æµé‡çŸ©é˜µ (N, 1)
                flow_matrix = np.zeros((self.num_nodes, 1), dtype=np.float32)
                for idx, flow in flow_dict.items():
                    if idx < self.num_nodes:
                        flow_matrix[idx, 0] = flow
                
                # è§£æç¨€ç–å†å²çŸ©é˜µ
                history_dict = {}
                if record['history_matrix_sparse']:
                    for item in record['history_matrix_sparse'].split(';'):
                        if ':' in item:
                            parts = item.split(':')
                            if len(parts) == 2:
                                idx_str, history_str = parts
                                idx = int(idx_str)
                                # history_str æ ¼å¼: "5;3;2;1;0;0;8;15;12;10;8;6"
                                history_values = [float(v) for v in history_str.split(';')]
                                history_dict[idx] = history_values
                
                # æ„å»ºå¯†é›†å†å²çŸ©é˜µ (T, N, 1)
                history_matrix = np.zeros((self.input_len, self.num_nodes, 1), dtype=np.float32)
                for idx, history in history_dict.items():
                    if idx < self.num_nodes:
                        # å–æœ€å input_len ä¸ªå€¼
                        history = history[-self.input_len:]
                        for t in range(len(history)):
                            history_matrix[t, idx, 0] = history[t]
                
                # è§£ææ—¶é—´ç‰¹å¾
                if record['time_features']:
                    time_feat = [float(x) for x in record['time_features'].split()]
                    # å– [day_of_week, hour]ï¼ˆåŸå§‹ä»£ç æ ¼å¼ï¼‰
                    if len(time_feat) >= 5:
                        day_of_week = time_feat[4]  # day_of_week
                        hour = time_feat[1]  # hour
                        time_feat_vec = [day_of_week, hour]
                    else:
                        time_feat_vec = [0, 0]
                else:
                    time_feat_vec = [0, 0]
                
                self.time_windows.append(time_window)
                self.flow_matrices.append(flow_matrix)
                self.history_matrices.append(history_matrix)
                self.time_features.append(time_feat_vec)
        
        log_string(self.log, f"   âœ… æ—¶é—´åºåˆ—æ•°æ®åŠ è½½å®Œæˆ: {len(self.time_windows)} ä¸ªæ—¶é—´çª—å£")
        
        # æ•°æ®éªŒè¯
        assert len(self.time_windows) > 0, "æ—¶é—´çª—å£æ•°é‡ä¸º0"
        assert len(self.time_windows) == len(self.flow_matrices), "æ•°æ®é•¿åº¦ä¸åŒ¹é…"
        
        # ç»Ÿè®¡æµé‡ä¿¡æ¯
        all_flows = np.concatenate([fm.flatten() for fm in self.flow_matrices])
        non_zero = all_flows[all_flows > 0]
        if len(non_zero) > 0:
            log_string(self.log, f"   ğŸ“Š æµé‡ç»Ÿè®¡:")
            log_string(self.log, f"      - éé›¶å€¼æ¯”ä¾‹: {len(non_zero)/len(all_flows)*100:.2f}%")
            log_string(self.log, f"      - æµé‡èŒƒå›´: [{non_zero.min():.2f}, {non_zero.max():.2f}]")
            log_string(self.log, f"      - å¹³å‡æµé‡: {non_zero.mean():.2f}")
    
    def __len__(self):
        """
        æ ·æœ¬æ•° = æ—¶é—´çª—å£æ•° - (input_len + output_len - 1)
        éœ€è¦æœ‰è¶³å¤Ÿçš„è¿ç»­æ—¶é—´çª—å£æ¥æ„å»ºè¾“å…¥å’Œè¾“å‡ºåºåˆ—
        """
        return max(0, len(self.time_windows) - self.input_len - self.output_len + 1)
    
    def __getitem__(self, idx):
        """
        è¿”å›ä¸€ä¸ªæ ·æœ¬
        
        Returns:
            X: (input_len, N, 1) - è¾“å…¥åºåˆ—
            Y: (output_len, N, 1) - è¾“å‡ºåºåˆ—ï¼ˆæ ‡ç­¾ï¼‰
            TE: (input_len, 2) - æ—¶é—´ç‰¹å¾
        """
        # è¾“å…¥åºåˆ—ï¼šä» idx å¼€å§‹çš„ input_len ä¸ªæ—¶é—´çª—å£
        X = np.stack([self.flow_matrices[i] for i in range(idx, idx + self.input_len)], axis=0)
        
        # è¾“å‡ºåºåˆ—ï¼šä» idx + input_len å¼€å§‹çš„ output_len ä¸ªæ—¶é—´çª—å£
        Y = np.stack([self.flow_matrices[i] for i in range(idx + self.input_len, idx + self.input_len + self.output_len)], axis=0)
        
        # æ—¶é—´ç‰¹å¾ï¼šè¾“å…¥åºåˆ—çš„æ—¶é—´ç‰¹å¾
        TE = np.array([self.time_features[i] for i in range(idx, idx + self.input_len)], dtype=np.float32)
        
        # éªŒè¯å½¢çŠ¶
        assert X.shape == (self.input_len, self.num_nodes, 1), f"X shape: {X.shape}"
        assert Y.shape == (self.output_len, self.num_nodes, 1), f"Y shape: {Y.shape}"
        assert TE.shape == (self.input_len, 2), f"TE shape: {TE.shape}"
        
        return {
            'X': X,
            'Y': Y,
            'TE': TE,
            'time_window': self.time_windows[idx]
        }


def create_preprocessed_dataloader(config, log=None):
    """
    åˆ›å»ºé¢„å¤„ç†æ•°æ®çš„ DataLoader
    
    Args:
        config: é…ç½®å­—å…¸ï¼ŒåŒ…å«:
            - odps_project: ODPS é¡¹ç›®å
            - odps_endpoint: ODPS endpoint
            - odps_table: æ—¶é—´åºåˆ—è¡¨å
            - odps_meta_table: å…ƒæ•°æ®è¡¨å
            - adcode: åŸå¸‚ä»£ç 
            - start_date: å¼€å§‹æ—¥æœŸ
            - end_date: ç»“æŸæ—¥æœŸ
            - batch_size: batch å¤§å°
            - num_workers: æ•°æ®åŠ è½½çº¿ç¨‹æ•°
            - input_len: è¾“å…¥åºåˆ—é•¿åº¦
            - output_len: è¾“å‡ºåºåˆ—é•¿åº¦
    
    Returns:
        (dataset, dataloader, node_locations)
    """
    from odps import ODPS
    
    # è·å– ODPS å‡­è¯
    access_id = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
    secret = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
    
    assert access_id and secret, "ç¼ºå°‘ ODPS å‡­è¯ç¯å¢ƒå˜é‡"
    
    # è¿æ¥ ODPS
    odps = ODPS(
        access_id, 
        secret, 
        config['odps_project'],
        endpoint=config.get('odps_endpoint', 'http://service-corp.odps.aliyun-inc.com/api')
    )
    
    # åˆ›å»ºæ•°æ®é›†
    dataset = PreprocessedODPSDataset(
        odps_instance=odps,
        table_name=config['odps_table'],
        meta_table_name=config['odps_meta_table'],
        adcode=config['adcode'],
        start_date=config['start_date'],
        end_date=config['end_date'],
        input_len=config.get('input_len', 12),
        output_len=config.get('output_len', 12),
        log=log
    )
    
    # åˆ›å»º DataLoader
    dataloader = DataLoader(
        dataset,
        batch_size=config.get('batch_size', 64),
        shuffle=True,
        num_workers=0,  # ODPS æ•°æ®å·²ç»åœ¨å†…å­˜ä¸­ï¼Œä¸éœ€è¦å¤šçº¿ç¨‹
        pin_memory=True
    )
    
    return dataset, dataloader, dataset.node_locations


if __name__ == '__main__':
    """æµ‹è¯•é¢„å¤„ç†æ•°æ®åŠ è½½å™¨"""
    
    config = {
        'odps_project': 'autonavi_traffic_report',
        'odps_endpoint': 'http://service-corp.odps.aliyun-inc.com/api',
        'odps_table': 'tb_inter_traffic_timeseries',
        'odps_meta_table': 'tb_inter_node_metadata',
        'adcode': '650100',
        'start_date': '20250919',
        'end_date': '20250925',
        'batch_size': 32,
        'num_workers': 0,
        'input_len': 12,
        'output_len': 12
    }
    
    print("=" * 80)
    print("æµ‹è¯•é¢„å¤„ç†æ•°æ®åŠ è½½å™¨")
    print("=" * 80)
    
    dataset, dataloader, locations = create_preprocessed_dataloader(config)
    
    print(f"\næ•°æ®é›†ä¿¡æ¯:")
    print(f"  - èŠ‚ç‚¹æ•°: {dataset.num_nodes}")
    print(f"  - æ ·æœ¬æ•°: {len(dataset)}")
    print(f"  - Batch æ•°: {len(dataloader)}")
    
    print(f"\nä½ç½®ä¿¡æ¯:")
    print(f"  - Shape: {locations.shape}")
    print(f"  - èŒƒå›´: [{locations.min():.4f}, {locations.max():.4f}]")
    
    print(f"\nåŠ è½½å‰ 3 ä¸ª batch:")
    for i, batch in enumerate(dataloader):
        if i >= 3:
            break
        
        X = batch['X']  # (batch, T, N, 1)
        Y = batch['Y']  # (batch, T', N, 1)
        TE = batch['TE']  # (batch, T, 2)
        
        print(f"\nBatch {i+1}:")
        print(f"  X shape: {X.shape}")
        print(f"  Y shape: {Y.shape}")
        print(f"  TE shape: {TE.shape}")
        
        # éªŒè¯æ•°æ®
        print(f"  X èŒƒå›´: [{X.min():.2f}, {X.max():.2f}]")
        print(f"  Y èŒƒå›´: [{Y.min():.2f}, {Y.max():.2f}]")
        print(f"  éé›¶æ¯”ä¾‹: {(X > 0).sum() / X.size * 100:.2f}%")
        
        # éªŒè¯æ‰€æœ‰èŠ‚ç‚¹éƒ½æœ‰æ•°æ®ï¼ˆéç¨€ç–ï¼‰
        nodes_with_data = (X.sum(axis=(0, 1)) > 0).sum()
        print(f"  æœ‰æ•°æ®çš„èŠ‚ç‚¹æ•°: {nodes_with_data} / {dataset.num_nodes}")
    
    print("\n" + "=" * 80)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)
