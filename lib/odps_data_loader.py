"""
ODPS æ•°æ®åŠ è½½å™¨
ä» MaxCompute è¡¨åŠ è½½äº¤é€šæµé‡æ•°æ®è¿›è¡Œè®­ç»ƒ
è¡¨å: autonavi_traffic_report.tb_inter_spatial_method_pretrain_data
"""
import os
import numpy as np
import pandas as pd
from odps import ODPS
from datetime import datetime, timedelta
from lib.utils import log_string


class ODPSDataLoader:
    """
    ä» ODPS åŠ è½½è®­ç»ƒæ•°æ®çš„æ•°æ®åŠ è½½å™¨
    """
    
    def __init__(self, config, log=None):
        """
        åˆå§‹åŒ– ODPS æ•°æ®åŠ è½½å™¨
        
        å‚æ•°:
            config (dict): é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«:
                - odps_project: ODPS é¡¹ç›®å
                - odps_endpoint: ODPS endpoint
                - odps_table: ODPS è¡¨åï¼ˆé»˜è®¤ä¸º tb_inter_spatial_method_pretrain_dataï¼‰
                - odps_meta_table: ODPS å…ƒæ•°æ®è¡¨åï¼ˆå¯é€‰ï¼ŒåŒ…å«èŠ‚ç‚¹ç»çº¬åº¦ä¿¡æ¯ï¼‰
                - adcode: è¡Œæ”¿åŒºåˆ’ä»£ç ï¼ˆå¦‚ '110000'ï¼‰
                - start_date: å¼€å§‹æ—¥æœŸ (æ ¼å¼: 'YYYYMMDD')
                - end_date: ç»“æŸæ—¥æœŸ (æ ¼å¼: 'YYYYMMDD')
                - input_len: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ 12ï¼‰
                - output_len: è¾“å‡ºåºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ 12ï¼‰
                - train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.7ï¼‰
                - val_ratio: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.1ï¼‰
                - test_ratio: æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.2ï¼‰
                - recur_times: KD-tree é€’å½’æ¬¡æ•°ï¼ˆé»˜è®¤ 1ï¼‰
                - spa_patchsize: ç©ºé—´ patch å¤§å°ï¼ˆé»˜è®¤ 4ï¼‰
            log: æ—¥å¿—æ–‡ä»¶å¯¹è±¡
        """
        self.config = config
        self.log = log
        
        # ODPS é…ç½®
        self.odps_project = config.get('odps_project', 'autonavi_traffic_report')
        self.odps_endpoint = config.get('odps_endpoint', 
                                       'http://service-corp.odps.aliyun-inc.com/api')
        self.odps_table = config.get('odps_table', 'tb_inter_spatial_method_pretrain_data')
        self.odps_meta_table = config.get('odps_meta_table', 'intersection_meta_aligned')  # é»˜è®¤ä½¿ç”¨å¯¹é½çš„å…ƒæ•°æ®è¡¨
        
        # âš ï¸ å…ƒæ•°æ®è¡¨æ˜¯å¿…é¡»çš„ï¼ˆç”¨äº KD-tree ç©ºé—´åˆ†ç»„ï¼‰
        if not self.odps_meta_table:
            raise ValueError(
                "å¿…é¡»æä¾› odps_meta_table å‚æ•°ï¼\n"
                "PatchSTG éœ€è¦èŠ‚ç‚¹ä½ç½®ä¿¡æ¯è¿›è¡Œç©ºé—´ KD-tree åˆ†ç»„ã€‚\n"
                "è¯·åœ¨é…ç½®ä¸­æ·»åŠ : odps_meta_table = 'intersection_meta_aligned'"
            )
        
        # æ•°æ®è¿‡æ»¤æ¡ä»¶
        self.adcode = config.get('adcode', None)
        self.start_date = config.get('start_date', None)
        self.end_date = config.get('end_date', None)
        self.limit = config.get('limit', None)  # å¯é€‰ï¼šé™åˆ¶æŸ¥è¯¢è¡Œæ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        
        # è®­ç»ƒå‚æ•°
        self.input_len = config.get('input_len', 12)
        self.output_len = config.get('output_len', 12)
        self.train_ratio = config.get('train_ratio', 0.6)  # 60% è®­ç»ƒé›†
        self.val_ratio = config.get('val_ratio', 0.2)     # 20% éªŒè¯é›†
        self.test_ratio = config.get('test_ratio', 0.2)   # 20% æµ‹è¯•é›†
        
        # ç©ºé—´ patching å‚æ•°
        self.recur_times = config.get('recur_times', 1)
        self.spa_patchsize = config.get('spa_patchsize', 4)
        
        # æ•°æ®é›†
        self.trainX = None
        self.trainY = None
        self.trainXTE = None
        self.trainYTE = None
        
        self.valX = None
        self.valY = None
        self.valXTE = None
        self.valYTE = None
        
        self.testX = None
        self.testY = None
        self.testXTE = None
        self.testYTE = None
        
        # å½’ä¸€åŒ–å‚æ•°
        self.mean = None
        self.std = None
        
        # èŠ‚ç‚¹ä¿¡æ¯
        self.node_list = None  # [(nds_id, next_nds_id), ...]
        self.node_num = 0
        self.node_to_idx = {}  # {(nds_id, next_nds_id): idx}
        self.node_locations = None  # èŠ‚ç‚¹ç»çº¬åº¦ (2, num_nodes): [lat, lng]
        
        # Patch ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ç©ºé—´ patchingï¼‰
        self.ori_parts_idx = None
        self.reo_parts_idx = None
        self.reo_all_idx = None
        
        self._loaded = False
        self._odps_client = None
    
    def _init_odps_client(self):
        """åˆå§‹åŒ– ODPS å®¢æˆ·ç«¯"""
        if self._odps_client is not None:
            return
        
        access_id = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
        secret = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
        
        if not access_id or not secret:
            raise RuntimeError(
                "ç¼ºå°‘ ODPS å‡­è¯ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡:\n"
                "  ALIBABA_CLOUD_ACCESS_KEY_ID\n"
                "  ALIBABA_CLOUD_ACCESS_KEY_SECRET"
            )
        
        self._odps_client = ODPS(
            access_id,
            secret,
            project=self.odps_project,
            endpoint=self.odps_endpoint
        )
        
        if self.log:
            log_string(self.log, f'ODPS client initialized for project: {self.odps_project}')
    
    def _build_query(self):
        """æ„å»º SQL æŸ¥è¯¢è¯­å¥"""
        where_clauses = []
        
        if self.adcode:
            where_clauses.append(f"adcode = '{self.adcode}'")
        
        if self.start_date and self.end_date:
            where_clauses.append(f"ds >= '{self.start_date}' AND ds <= '{self.end_date}'")
        elif self.start_date:
            where_clauses.append(f"ds >= '{self.start_date}'")
        elif self.end_date:
            where_clauses.append(f"ds <= '{self.end_date}'")
        
        where_clause = " AND ".join(where_clauses) if where_clauses else "1=1"
        
        # æ„å»º LIMIT å­å¥
        limit_clause = f"\nLIMIT {self.limit}" if self.limit else ""
        
        query = f"""
        SELECT 
            nds_id,
            next_nds_id,
            adcode,
            ds,
            passts_time,
            flow_label,
            time_feat,
            dym_feat_feat
        FROM {self.odps_table}
        WHERE {where_clause}
        ORDER BY nds_id, next_nds_id, passts_time{limit_clause}
        """
        
        return query
    
    def _parse_time_feat(self, time_feat_str):
        """
        è§£ææ—¶é—´ç‰¹å¾å­—ç¬¦ä¸²
        
        å‚æ•°:
            time_feat_str: "week hour minute day_type day month;..." (24æ®µ)
            
        è¿”å›:
            np.ndarray: shape (24, 6)
        """
        segments = time_feat_str.split(';')
        features = []
        
        for seg in segments:
            parts = seg.strip().split()
            if len(parts) == 6:
                features.append([int(p) for p in parts])
            else:
                # å¼‚å¸¸å¤„ç†ï¼šå¡«å……é»˜è®¤å€¼
                features.append([0, 0, 0, 0, 0, 0])
        
        # ç¡®ä¿æœ‰ 24 æ®µ
        while len(features) < 24:
            features.append([0, 0, 0, 0, 0, 0])
        
        return np.array(features[:24], dtype=np.float32)
    
    def _parse_dym_feat(self, dym_feat_str):
        """
        è§£æåŠ¨æ€æµé‡ç‰¹å¾å­—ç¬¦ä¸²
        
        å‚æ•°:
            dym_feat_str: "15;8;0;12;...;3" (24ä¸ªå€¼)
            
        è¿”å›:
            np.ndarray: shape (24,)
        """
        values = dym_feat_str.split(';')
        features = []
        
        for val in values:
            try:
                features.append(float(val.strip()))
            except:
                features.append(0.0)
        
        # ç¡®ä¿æœ‰ 24 ä¸ªå€¼
        while len(features) < 24:
            features.append(0.0)
        
        return np.array(features[:24], dtype=np.float32)
    
    def load_data(self):
        """
        ä» ODPS åŠ è½½æ•°æ®ï¼ˆæµå¼è¯»å–ç‰ˆæœ¬ï¼‰
        
        âœ… æ”¹è¿›ï¼šä½¿ç”¨ Table Iterator æµå¼è¯»å–ï¼Œé¿å…å†…å­˜æº¢å‡º
        """
        if self._loaded:
            if self.log:
                log_string(self.log, 'Data already loaded, skipping...')
            return
        
        if self.log:
            log_string(self.log, '\n------------ Loading Data from ODPS (Streaming) -------------')
            log_string(self.log, f'Project: {self.odps_project}')
            log_string(self.log, f'Table: {self.odps_table}')
            log_string(self.log, f'Adcode: {self.adcode}')
            log_string(self.log, f'Date range: {self.start_date} ~ {self.end_date}')
        
        # åˆå§‹åŒ– ODPS å®¢æˆ·ç«¯
        self._init_odps_client()
        
        # æ­¥éª¤ 1: å…ˆæŸ¥è¯¢èŠ‚ç‚¹åˆ—è¡¨ï¼ˆç”¨å°æŸ¥è¯¢è·å–å”¯ä¸€èŠ‚ç‚¹å¯¹ï¼‰
        if self.log:
            log_string(self.log, '\nStep 1: Loading node list...')
        self._load_node_list_from_odps()
        
        # æ­¥éª¤ 2: åŠ è½½èŠ‚ç‚¹ä½ç½®ä¿¡æ¯
        if self.log:
            log_string(self.log, '\nStep 2: Loading node locations...')
        self._load_node_locations()
        
        # æ­¥éª¤ 3: æµå¼è¯»å–æ•°æ®å¹¶å¤„ç†
        if self.log:
            log_string(self.log, '\nStep 3: Streaming data from ODPS...')
        self._stream_and_process_data()
        
        self._loaded = True
        
        if self.log:
            log_string(self.log, f'\nâœ… Data loading completed!')
            log_string(self.log, f'Train samples: {self.trainX.shape[0]}')
            log_string(self.log, f'Val samples: {self.valX.shape[0]}')
            log_string(self.log, f'Test samples: {self.testX.shape[0]}')
            log_string(self.log, f'Nodes: {self.node_num}')
            log_string(self.log, f'Mean: {self.mean:.4f}, Std: {self.std:.4f}')
            log_string(self.log, '------------ End -------------\n')
    
    def load_data_for_date_range(self, start_date, end_date):
        """
        ä¸ºæŒ‡å®šæ—¥æœŸèŒƒå›´åŠ è½½æ•°æ®ï¼ˆæ–¹æ¡ˆ 3ï¼šåˆ†æ‰¹åŠ è½½è®­ç»ƒï¼‰
        
        ğŸ“Œ ç”¨æ³•ï¼šåœ¨è®­ç»ƒå¾ªç¯ä¸­å¤šæ¬¡è°ƒç”¨ï¼Œæ¯æ¬¡åŠ è½½ä¸åŒæ—¥æœŸçš„æ•°æ®
        
        å‚æ•°:
            start_date (str): å¼€å§‹æ—¥æœŸ 'YYYYMMDD'
            end_date (str): ç»“æŸæ—¥æœŸ 'YYYYMMDD'
        
        ç¤ºä¾‹:
            # æ¯æ¬¡è®­ç»ƒåŠ è½½ 2 å¤©æ•°æ®
            for date_batch in date_chunks:
                data_loader.load_data_for_date_range('20250919', '20250920')
                trainX, trainY, trainXTE, trainYTE = data_loader.get_train_data()
                # è®­ç»ƒè¿™æ‰¹æ•°æ®...
                data_loader.clear_data()  # é‡Šæ”¾å†…å­˜
        """
        if self.log:
            log_string(self.log, f'\nğŸ”„ Loading data for date range: {start_date} ~ {end_date}')
        
        # ä¸´æ—¶ä¿®æ”¹é…ç½®çš„æ—¥æœŸèŒƒå›´
        original_start = self.start_date
        original_end = self.end_date
        self.start_date = start_date
        self.end_date = end_date
        
        # å¦‚æœæ˜¯é¦–æ¬¡åŠ è½½ï¼Œéœ€è¦åˆå§‹åŒ–å®¢æˆ·ç«¯å’ŒèŠ‚ç‚¹åˆ—è¡¨
        if self._odps_client is None:
            self._init_odps_client()
        
        if self.node_list is None:
            if self.log:
                log_string(self.log, 'Step 1: Loading node list (first time)...')
            # ä½¿ç”¨åŸå§‹å®Œæ•´æ—¥æœŸèŒƒå›´è·å–èŠ‚ç‚¹åˆ—è¡¨
            self.start_date = original_start
            self.end_date = original_end
            self._load_node_list_from_odps()
            # æ¢å¤å½“å‰æ‰¹æ¬¡çš„æ—¥æœŸèŒƒå›´
            self.start_date = start_date
            self.end_date = end_date
        
        if self.node_locations is None:
            if self.log:
                log_string(self.log, 'Step 2: Loading node locations (first time)...')
            self._load_node_locations()
        
        # æµå¼è¯»å–å½“å‰æ—¥æœŸèŒƒå›´çš„æ•°æ®
        if self.log:
            log_string(self.log, 'Step 3: Streaming data for this date range...')
        self._stream_and_process_data()
        
        # æ¢å¤åŸå§‹æ—¥æœŸé…ç½®
        self.start_date = original_start
        self.end_date = original_end
        self._loaded = True
        
        if self.log:
            log_string(self.log, f'âœ… Loaded {self.trainX.shape[0]} samples for {start_date} ~ {end_date}\n')
    
    def clear_data(self):
        """
        æ¸…ç©ºå·²åŠ è½½çš„æ•°æ®ï¼Œé‡Šæ”¾å†…å­˜
        
        ğŸ“Œ ç”¨äºåˆ†æ‰¹åŠ è½½åœºæ™¯ï¼šè®­ç»ƒå®Œå½“å‰æ‰¹æ¬¡åé‡Šæ”¾å†…å­˜
        """
        self.trainX = None
        self.trainY = None
        self.trainXTE = None
        self.trainYTE = None
        self.valX = None
        self.valY = None
        self.valXTE = None
        self.valYTE = None
        self.testX = None
        self.testY = None
        self.testXTE = None
        self.testYTE = None
        self._loaded = False
        
        if self.log:
            log_string(self.log, 'ğŸ—‘ï¸  Data cleared, memory released')
    
    def _load_node_list_from_odps(self):
        """
        ä» ODPS æŸ¥è¯¢å”¯ä¸€çš„èŠ‚ç‚¹åˆ—è¡¨
        
        ä½¿ç”¨ DISTINCT æŸ¥è¯¢ï¼Œæ•°æ®é‡å°ï¼Œä¸ä¼šæœ‰å†…å­˜é—®é¢˜
        """
        query = f"""
        SELECT DISTINCT 
            nds_id,
            next_nds_id
        FROM {self.odps_table}
        WHERE 1=1
        """
        
        if self.adcode:
            query += f" AND adcode = '{self.adcode}'"
        
        if self.start_date and self.end_date:
            query += f" AND ds >= '{self.start_date}' AND ds <= '{self.end_date}'"
        elif self.start_date:
            query += f" AND ds >= '{self.start_date}'"
        elif self.end_date:
            query += f" AND ds <= '{self.end_date}'"
        
        if self.log:
            log_string(self.log, f'   Querying unique nodes...')
        
        # æ‰§è¡ŒæŸ¥è¯¢
        with self._odps_client.execute_sql(query).open_reader() as reader:
            node_pairs = [(record[0], record[1]) for record in reader]
        
        self.node_list = node_pairs
        self.node_num = len(self.node_list)
        self.node_to_idx = {node: idx for idx, node in enumerate(self.node_list)}
        
        if self.log:
            log_string(self.log, f'   âœ… Found {self.node_num} unique node pairs')
    
    def _stream_and_process_data(self):
        """
        æµå¼è¯»å– ODPS æ•°æ®å¹¶å¤„ç†
        
        âœ… æ ¸å¿ƒæ”¹è¿›ï¼šä½¿ç”¨ Table API ç›´æ¥è¯»å–ï¼Œæ”¯æŒåˆ†ç‰‡å’Œæµå¼å¤„ç†
        """
        # æ„å»ºæŸ¥è¯¢ï¼ˆç”¨äºè·å–è¡¨ï¼‰
        query = self._build_query()
        
        if self.log:
            log_string(self.log, f'   Executing streaming query...')
            log_string(self.log, f'   Query:\n{query}')
        
        # æ–¹æ¡ˆï¼šä½¿ç”¨ execute_sql çš„ open_reader ä½†åˆ†æ‰¹è¯»å–
        # open_reader è¿”å›ä¸€ä¸ªè¿­ä»£å™¨ï¼Œæˆ‘ä»¬å¯ä»¥åˆ†æ‰¹å¤„ç†
        
        chunk_size = 100000  # æ¯æ‰¹å¤„ç† 10 ä¸‡æ¡è®°å½•
        total_records = 0
        
        # ç”¨äºç´¯ç§¯æ—¶é—´åºåˆ—æ•°æ®çš„å­—å…¸
        # key: time_minute, value: {node_idx: flow_value}
        time_series_dict = {}
        
        if self.log:
            log_string(self.log, f'   Reading data in chunks of {chunk_size} records...')
        
        with self._odps_client.execute_sql(query).open_reader() as reader:
            chunk_records = []
            
            for record in reader:
                chunk_records.append(record.values)
                
                # è¾¾åˆ°æ‰¹æ¬¡å¤§å°ï¼Œå¤„ç†è¿™æ‰¹æ•°æ®
                if len(chunk_records) >= chunk_size:
                    self._process_chunk(chunk_records, time_series_dict)
                    total_records += len(chunk_records)
                    
                    if self.log:
                        log_string(self.log, f'   Processed {total_records} records...')
                    
                    chunk_records = []
            
            # å¤„ç†æœ€åä¸€æ‰¹
            if chunk_records:
                self._process_chunk(chunk_records, time_series_dict)
                total_records += len(chunk_records)
        
        if self.log:
            log_string(self.log, f'   âœ… Total records processed: {total_records}')
            log_string(self.log, f'   Unique time steps: {len(time_series_dict)}')
        
        if total_records == 0:
            raise ValueError("No data loaded from ODPS. Check your filter conditions.")
        
        # è½¬æ¢ä¸º DataFrame å¹¶ç»§ç»­åç»­å¤„ç†
        if self.log:
            log_string(self.log, '   Converting to time series format...')
        
        self._build_time_series_from_dict(time_series_dict)
    
    def _process_chunk(self, records, time_series_dict):
        """
        å¤„ç†ä¸€æ‰¹è®°å½•ï¼Œç´¯ç§¯åˆ°æ—¶é—´åºåˆ—å­—å…¸ä¸­
        
        å‚æ•°:
            records: è®°å½•åˆ—è¡¨
            time_series_dict: ç´¯ç§¯çš„æ—¶é—´åºåˆ—å­—å…¸
        """
        columns = ['nds_id', 'next_nds_id', 'adcode', 'ds', 'passts_time', 
                  'flow_label', 'time_feat', 'dym_feat_feat']
        df_chunk = pd.DataFrame(records, columns=columns)
        
        # è½¬æ¢æ—¶é—´æˆ³
        df_chunk['timestamp'] = pd.to_datetime(df_chunk['passts_time'])
        df_chunk['time_minute'] = df_chunk['timestamp'].dt.floor('1min')
        
        # æ·»åŠ èŠ‚ç‚¹ç´¢å¼•
        df_chunk['node_idx'] = df_chunk.apply(
            lambda row: self.node_to_idx.get((row['nds_id'], row['next_nds_id']), -1), 
            axis=1
        )
        
        # è¿‡æ»¤æ‰æœªçŸ¥èŠ‚ç‚¹
        df_chunk = df_chunk[df_chunk['node_idx'] != -1]
        
        # ç´¯ç§¯åˆ°å­—å…¸ä¸­
        for _, row in df_chunk.iterrows():
            time_key = row['time_minute']
            node_idx = row['node_idx']
            flow_value = row['flow_label']
            
            if time_key not in time_series_dict:
                time_series_dict[time_key] = {}
            
            # å¦‚æœåŒä¸€èŠ‚ç‚¹åŒä¸€æ—¶é—´æœ‰å¤šæ¡è®°å½•ï¼Œå–å¹³å‡
            if node_idx in time_series_dict[time_key]:
                time_series_dict[time_key][node_idx] = (
                    time_series_dict[time_key][node_idx] + flow_value
                ) / 2
            else:
                time_series_dict[time_key][node_idx] = flow_value
    
    def _build_time_series_from_dict(self, time_series_dict):
        """
        ä»æ—¶é—´åºåˆ—å­—å…¸æ„å»ºæœ€ç»ˆçš„è®­ç»ƒæ•°æ®
        
        å‚æ•°:
            time_series_dict: {time_minute: {node_idx: flow_value}}
        """
        # æ’åºæ—¶é—´ç‚¹
        sorted_times = sorted(time_series_dict.keys())
        num_times = len(sorted_times)
        
        if self.log:
            log_string(self.log, f'   Time range: {sorted_times[0]} ~ {sorted_times[-1]}')
            log_string(self.log, f'   Time steps: {num_times}')
        
        # æ„å»ºæµé‡çŸ©é˜µ (num_times, num_nodes)
        flow_matrix = np.zeros((num_times, self.node_num), dtype=np.float32)
        
        for t_idx, time_key in enumerate(sorted_times):
            node_flows = time_series_dict[time_key]
            for node_idx, flow_value in node_flows.items():
                flow_matrix[t_idx, node_idx] = flow_value
        
        if self.log:
            non_zero_ratio = (flow_matrix > 0).sum() / flow_matrix.size * 100
            log_string(self.log, f'   Flow matrix shape: {flow_matrix.shape}')
            log_string(self.log, f'   Non-zero ratio: {non_zero_ratio:.2f}%')
        
        # æ„å»ºæ—¶é—´ç‰¹å¾
        time_features = []
        for timestamp in sorted_times:
            hour = timestamp.hour
            day_of_week = timestamp.dayofweek
            tod = hour / 24.0
            dow = day_of_week / 7.0
            time_features.append([tod, dow])
        
        time_features = np.array(time_features, dtype=np.float32)
        
        # ç”Ÿæˆæ ·æœ¬
        if self.log:
            log_string(self.log, '   Generating samples with sliding window...')
        
        num_samples = num_times - self.input_len - self.output_len + 1
        
        if num_samples <= 0:
            raise ValueError(
                f"Not enough time steps to generate samples!\n"
                f"  Time steps: {num_times}\n"
                f"  Required: input_len ({self.input_len}) + output_len ({self.output_len}) = {self.input_len + self.output_len}\n"
                f"  Please use a longer date range."
            )
        
        # é¢„åˆ†é…æ•°ç»„
        X_data = np.zeros((num_samples, self.input_len, self.node_num, 1), dtype=np.float32)
        Y_data = np.zeros((num_samples, self.output_len, self.node_num, 1), dtype=np.float32)
        XTE_data = np.zeros((num_samples, self.input_len, 2), dtype=np.float32)
        YTE_data = np.zeros((num_samples, self.output_len, 2), dtype=np.float32)
        
        # æ»‘åŠ¨çª—å£ç”Ÿæˆæ ·æœ¬
        for i in range(num_samples):
            X_data[i, :, :, 0] = flow_matrix[i:i+self.input_len]
            XTE_data[i] = time_features[i:i+self.input_len]
            Y_data[i, :, :, 0] = flow_matrix[i+self.input_len:i+self.input_len+self.output_len]
            YTE_data[i] = time_features[i+self.input_len:i+self.input_len+self.output_len]
        
        if self.log:
            log_string(self.log, f'   âœ… Generated {num_samples} samples')
        
        # éªŒè¯æ•°æ®
        if np.any(np.isnan(X_data)) or np.any(np.isinf(X_data)):
            raise ValueError("X_data contains NaN or Inf values!")
        if np.any(np.isnan(Y_data)) or np.any(np.isinf(Y_data)):
            raise ValueError("Y_data contains NaN or Inf values!")
        
        # è®¡ç®—å½’ä¸€åŒ–å‚æ•°
        num_train = int(num_samples * self.train_ratio)
        train_data = X_data[:num_train]
        train_nonzero = train_data[train_data > 0]
        
        if len(train_nonzero) > 0:
            self.mean = np.mean(train_nonzero)
            self.std = np.std(train_nonzero)
        else:
            self.mean = 0.0
            self.std = 1.0
        
        if self.std < 1e-6:
            self.std = 1.0
        
        if self.log:
            log_string(self.log, f'   Normalization: mean={self.mean:.4f}, std={self.std:.4f}')
        
        # åˆ’åˆ†æ•°æ®é›†
        num_val = int(num_samples * self.val_ratio)
        num_test = int(num_samples * self.test_ratio)
        
        self.trainX = X_data[:num_train]
        self.trainY = Y_data[:num_train]
        self.trainXTE = XTE_data[:num_train]
        self.trainYTE = YTE_data[:num_train]
        
        self.valX = X_data[num_train:num_train+num_val]
        self.valY = Y_data[num_train:num_train+num_val]
        self.valXTE = XTE_data[num_train:num_train+num_val]
        self.valYTE = YTE_data[num_train:num_train+num_val]
        
        self.testX = X_data[num_train+num_val:num_train+num_val+num_test]
        self.testY = Y_data[num_train+num_val:num_train+num_val+num_test]
        self.testXTE = XTE_data[num_train+num_val:num_train+num_val+num_test]
        self.testYTE = YTE_data[num_train+num_val:num_train+num_val+num_test]
        
        if self.log:
            log_string(self.log, f'   âœ… Dataset split: Train={num_train}, Val={num_val}, Test={num_test}')
        
        # åˆ›å»ºç©ºé—´ patch
        self._create_spatial_patches(self.trainX)

    def _load_node_locations(self):
        """
        ä» ODPS å…ƒæ•°æ®è¡¨åŠ è½½è·¯å£çš„ç»çº¬åº¦ä¿¡æ¯ï¼ˆå¿…é¡»ï¼‰
        
        âš ï¸ ç»çº¬åº¦æ˜¯å¿…é¡»çš„ï¼Œç”¨äº KD-tree ç©ºé—´åˆ†ç»„
        
        æ•°æ®å…³ç³»:
        - (nds_id, next_nds_id) è¡¨ç¤ºä¸€ä¸ªè½¬å‘æµ
        - æ¯ä¸ªè½¬å‘æµå¯¹åº”ä¸€ä¸ªè·¯å£ inter_id
        - è·¯å£ inter_id æœ‰ç»çº¬åº¦åæ ‡
        
        å…ƒæ•°æ®è¡¨åº”åŒ…å«ä»¥ä¸‹å­—æ®µ:
        - nds_id: è½¬å‘å‰çš„è·¯æ®µ ID
        - next_nds_id: è½¬å‘åçš„è·¯æ®µ ID
        - inter_id: è·¯å£ ID
        - lat: è·¯å£çº¬åº¦
        - lng: è·¯å£ç»åº¦
        """
        
        if self.log:
            log_string(self.log, f'\nğŸ“ Loading node locations from: {self.odps_meta_table}')
        
        # æ„å»ºæŸ¥è¯¢ï¼šè·å–æ‰€æœ‰è½¬å‘æµå¯¹åº”çš„è·¯å£ä½ç½®
        query = f"""
        SELECT 
            nds_id,
            next_nds_id,
            inter_id,
            lat,
            lng
        FROM {self.odps_meta_table}
        WHERE 1=1
        """
        
        # å¦‚æœæœ‰ adcode è¿‡æ»¤ï¼Œä¹Ÿåº”ç”¨åˆ°å…ƒæ•°æ®è¡¨
        if self.adcode:
            query += f" AND adcode = '{self.adcode}'"
        
        if self.log:
            log_string(self.log, f'Executing meta query:\n{query}')
        
        # æ‰§è¡ŒæŸ¥è¯¢
        with self._odps_client.execute_sql(query).open_reader() as reader:
            meta_records = [record.values for record in reader]
            meta_df = pd.DataFrame(meta_records, 
                                  columns=['nds_id', 'next_nds_id', 'inter_id', 
                                         'lat', 'lng'])
        
        if len(meta_df) == 0:
            raise RuntimeError(
                f"âŒ å…ƒæ•°æ®è¡¨ {self.odps_meta_table} ä¸­æ²¡æœ‰æ‰¾åˆ°ä½ç½®æ•°æ®ï¼\n"
                f"   è¿‡æ»¤æ¡ä»¶: adcode = '{self.adcode}'\n"
                f"   PatchSTG éœ€è¦èŠ‚ç‚¹ä½ç½®è¿›è¡Œç©ºé—´åˆ†ç»„ã€‚"
            )
        
        if self.log:
            unique_intersections = meta_df['inter_id'].nunique()
            log_string(self.log, f'   Found {len(meta_df)} turn flows across {unique_intersections} intersections')
        
        # åˆ›å»ºä½ç½®æ•°ç»„ (2, num_nodes): [lat, lng]
        self.node_locations = np.zeros((2, self.node_num), dtype=np.float32)
        
        # å¡«å……æ¯ä¸ªè½¬å‘æµï¼ˆèŠ‚ç‚¹å¯¹ï¼‰çš„ä½ç½®ï¼ˆä½¿ç”¨å¯¹åº”è·¯å£çš„ä½ç½®ï¼‰
        missing_count = 0
        missing_nodes = []
        
        for idx, (nds_id, next_nds_id) in enumerate(self.node_list):
            # æŸ¥æ‰¾å¯¹åº”çš„è·¯å£ä½ç½®
            location = meta_df[
                (meta_df['nds_id'] == nds_id) & 
                (meta_df['next_nds_id'] == next_nds_id)
            ]
            
            if len(location) > 0:
                lat = location.iloc[0]['lat']
                lng = location.iloc[0]['lng']
                
                # æ£€æŸ¥ç»çº¬åº¦æ˜¯å¦æœ‰æ•ˆ
                if lat is None or lng is None or lat == 0 or lng == 0:
                    missing_count += 1
                    missing_nodes.append((nds_id, next_nds_id))
                    self.node_locations[0, idx] = 0.0
                    self.node_locations[1, idx] = 0.0
                else:
                    self.node_locations[0, idx] = lat
                    self.node_locations[1, idx] = lng
            else:
                missing_count += 1
                missing_nodes.append((nds_id, next_nds_id))
                self.node_locations[0, idx] = 0.0
                self.node_locations[1, idx] = 0.0
        
        # è®¡ç®—è¦†ç›–ç‡
        coverage = (self.node_num - missing_count) * 100.0 / self.node_num
        
        if self.log:
            log_string(self.log, f'   âœ… Loaded locations for {self.node_num - missing_count}/{self.node_num} nodes')
            log_string(self.log, f'   ğŸ“Š Coverage: {coverage:.2f}%')
        
        # âš ï¸ å¦‚æœè¦†ç›–ç‡å¤ªä½ï¼ŒæŠ¥é”™
        if coverage < 50.0:
            raise RuntimeError(
                f"âŒ ä½ç½®è¦†ç›–ç‡å¤ªä½: {coverage:.2f}%\n"
                f"   åªæœ‰ {self.node_num - missing_count}/{self.node_num} ä¸ªèŠ‚ç‚¹æœ‰æœ‰æ•ˆä½ç½®ã€‚\n"
                f"   PatchSTG éœ€è¦è‡³å°‘ 50% çš„èŠ‚ç‚¹æœ‰ä½ç½®ä¿¡æ¯æ‰èƒ½è¿›è¡Œç©ºé—´åˆ†ç»„ã€‚\n"
                f"   è¯·æ£€æŸ¥å…ƒæ•°æ®è¡¨æ˜¯å¦åŒ…å«æ‰€æœ‰èŠ‚ç‚¹çš„ä½ç½®ä¿¡æ¯ã€‚"
            )
        
        if missing_count > 0:
            if self.log:
                log_string(self.log, f'   âš ï¸  Warning: {missing_count} nodes have missing locations')
                if missing_count <= 10:
                    log_string(self.log, f'   Missing nodes: {missing_nodes[:10]}')

    def _create_spatial_patches(self, train_data):
        """
        åˆ›å»ºç©ºé—´ patch ç´¢å¼•ï¼ˆä½¿ç”¨ KD-treeï¼‰
        
        âš ï¸ å¿…é¡»æœ‰èŠ‚ç‚¹ä½ç½®ä¿¡æ¯æ‰èƒ½è¿›è¡Œç©ºé—´åˆ†ç»„
        """
        if self.log:
            log_string(self.log, '\nğŸŒ³ Creating spatial patches using KD-tree...')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä½ç½®ä¿¡æ¯
        if self.node_locations is None:
            raise RuntimeError(
                "âŒ æ²¡æœ‰èŠ‚ç‚¹ä½ç½®ä¿¡æ¯ï¼Œæ— æ³•åˆ›å»ºç©ºé—´ patchï¼\n"
                "   è¯·ç¡®ä¿å·²åŠ è½½å…ƒæ•°æ®è¡¨ã€‚"
            )
        
        # æ£€æŸ¥ä½ç½®ä¿¡æ¯æ˜¯å¦æœ‰æ•ˆ
        valid_locations = (self.node_locations != 0).any(axis=0).sum()
        if valid_locations == 0:
            raise RuntimeError(
                "âŒ æ‰€æœ‰èŠ‚ç‚¹çš„ä½ç½®ä¿¡æ¯éƒ½æ— æ•ˆï¼ˆå…¨ä¸º0ï¼‰ï¼\n"
                "   è¯·æ£€æŸ¥å…ƒæ•°æ®è¡¨ä¸­çš„ lat, lng å­—æ®µã€‚"
            )
        
        try:
            # å¯¼å…¥åŸæœ‰çš„ patching å‡½æ•°
            from lib.utils import construct_adj, reorderData
            from sklearn.neighbors import KDTree
            
            if self.log:
                log_string(self.log, f'   Node locations shape: {self.node_locations.shape}')
                log_string(self.log, f'   Valid locations: {valid_locations}/{self.node_num}')
                log_string(self.log, f'   Lat range: [{self.node_locations[0].min():.4f}, {self.node_locations[0].max():.4f}]')
                log_string(self.log, f'   Lng range: [{self.node_locations[1].min():.4f}, {self.node_locations[1].max():.4f}]')
            
            # ä½¿ç”¨ KD-tree è¿›è¡Œç©ºé—´åˆ’åˆ†
            tree = KDTree(self.node_locations.T)  # (num_nodes, 2)
            
            # é€’å½’åˆ’åˆ†
            def recursive_split(indices, depth=0):
                if depth >= self.recur_times or len(indices) <= self.spa_patchsize:
                    return [indices]
                
                # æ‰¾åˆ°ä¸­ä½æ•°å¹¶åˆ†å‰²
                coords = self.node_locations[:, indices]
                axis = depth % 2  # 0 for lat, 1 for lng
                median_idx = len(indices) // 2
                sorted_indices = indices[np.argsort(coords[axis, :])]
                
                left = sorted_indices[:median_idx]
                right = sorted_indices[median_idx:]
                
                return recursive_split(left, depth+1) + recursive_split(right, depth+1)
            
            parts_idx = recursive_split(np.arange(self.node_num))
            
            if self.log:
                log_string(self.log, f'   âœ… Created {len(parts_idx)} spatial patches')
                patch_sizes = [len(p) for p in parts_idx]
                log_string(self.log, f'   Patch size range: [{min(patch_sizes)}, {max(patch_sizes)}]')
                log_string(self.log, f'   Average patch size: {np.mean(patch_sizes):.1f}')
            
            # æ„é€ é‚»æ¥çŸ©é˜µç”¨äºè¡¥é½
            if self.log:
                log_string(self.log, '   Constructing adjacency matrix...')
            # construct_adj æœŸæœ› (time_steps, nodes, 1)ï¼Œä½† train_data æ˜¯ (samples, time_steps, nodes, 1)
            # æˆ‘ä»¬å°†æ‰€æœ‰æ ·æœ¬æ‹¼æ¥æˆä¸€ä¸ªé•¿æ—¶é—´åºåˆ—
            train_data_concat = train_data.reshape(-1, self.node_num, 1)  # (samples*time_steps, nodes, 1)
            adj = construct_adj(train_data_concat, self.node_num)
            
            # è·å–æœ€å¤§ patch é•¿åº¦
            mxlen = max([len(p) for p in parts_idx])
            
            # é‡æ’å¹¶è¡¥é½
            if self.log:
                log_string(self.log, '   Reordering patches...')
            self.ori_parts_idx, self.reo_parts_idx, self.reo_all_idx = reorderData(
                parts_idx, mxlen, adj, self.spa_patchsize
            )
            
            if self.log:
                log_string(self.log, '   âœ… Spatial patching completed')
            
        except ImportError as e:
            raise RuntimeError(
                f"âŒ ç¼ºå°‘å¿…è¦çš„åº“: {str(e)}\n"
                f"   è¯·å®‰è£…: pip install scikit-learn"
            )
        except Exception as e:
            raise RuntimeError(
                f"âŒ ç©ºé—´ patching å¤±è´¥: {str(e)}\n"
                f"   è¿™æ˜¯åˆ›å»º KD-tree ç©ºé—´åˆ†ç»„æ—¶çš„é”™è¯¯ã€‚"
            )
    
    def get_train_data(self):
        """è·å–è®­ç»ƒæ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return self.trainX, self.trainY, self.trainXTE, self.trainYTE
    
    def get_val_data(self):
        """è·å–éªŒè¯æ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return self.valX, self.valY, self.valXTE, self.valYTE
    
    def get_test_data(self):
        """è·å–æµ‹è¯•æ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return self.testX, self.testY, self.testXTE, self.testYTE
    
    def get_normalization_params(self):
        """è·å–å½’ä¸€åŒ–å‚æ•°"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return self.mean, self.std
    
    def get_patch_indices(self):
        """è·å– patch ç´¢å¼•"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return self.ori_parts_idx, self.reo_parts_idx, self.reo_all_idx
    
    def shuffle_train_data(self, seed=None):
        """æ‰“ä¹±è®­ç»ƒæ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        
        if seed is not None:
            np.random.seed(seed)
        
        num_train = self.trainX.shape[0]
        permutation = np.random.permutation(num_train)
        
        self.trainX = self.trainX[permutation]
        self.trainY = self.trainY[permutation]
        self.trainXTE = self.trainXTE[permutation]
        
        if self.log:
            log_string(self.log, 'Training data shuffled')
    
    def normalize_data(self, data):
        """å½’ä¸€åŒ–æ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return (data - self.mean) / self.std
    
    def denormalize_data(self, data):
        """åå½’ä¸€åŒ–æ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return data * self.std + self.mean
    
    def get_data_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        
        return {
            'train_samples': self.trainX.shape[0],
            'val_samples': self.valX.shape[0],
            'test_samples': self.testX.shape[0],
            'input_shape': self.trainX.shape[1:],
            'output_shape': self.trainY.shape[1:],
            'mean': float(self.mean),
            'std': float(self.std),
            'num_nodes': self.node_num,
            'node_list': self.node_list[:10] if len(self.node_list) > 10 else self.node_list,
            'adcode': self.adcode,
            'date_range': f'{self.start_date} ~ {self.end_date}'
        }
