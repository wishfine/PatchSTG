"""
ODPS æ•°æ®åŠ è½½å™¨
ä» MaxCompute è¡¨åŠ è½½äº¤é€šæµé‡æ•°æ®è¿›è¡Œè®­ç»ƒ
è¡¨å: autonavi_traffic_report.tb_inter_spatial_method_pretrain_data
"""
import os
import numpy as np
import pandas as pd
from odps import ODPS
from datetime import datetime, timedelta
from lib.utils import log_string


class ODPSDataLoader:
    """
    ä» ODPS åŠ è½½è®­ç»ƒæ•°æ®çš„æ•°æ®åŠ è½½å™¨
    """
    
    def __init__(self, config, log=None):
        """
        åˆå§‹åŒ– ODPS æ•°æ®åŠ è½½å™¨
        
        å‚æ•°:
            config (dict): é…ç½®å­—å…¸ï¼Œéœ€åŒ…å«:
                - odps_project: ODPS é¡¹ç›®å
                - odps_endpoint: ODPS endpoint
                - odps_table: ODPS è¡¨åï¼ˆé»˜è®¤ä¸º tb_inter_spatial_method_pretrain_dataï¼‰
                - odps_meta_table: ODPS å…ƒæ•°æ®è¡¨åï¼ˆå¯é€‰ï¼ŒåŒ…å«èŠ‚ç‚¹ç»çº¬åº¦ä¿¡æ¯ï¼‰
                - adcode: è¡Œæ”¿åŒºåˆ’ä»£ç ï¼ˆå¦‚ '110000'ï¼‰
                - start_date: å¼€å§‹æ—¥æœŸ (æ ¼å¼: 'YYYYMMDD')
                - end_date: ç»“æŸæ—¥æœŸ (æ ¼å¼: 'YYYYMMDD')
                - input_len: è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ 12ï¼‰
                - output_len: è¾“å‡ºåºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ 12ï¼‰
                - train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.7ï¼‰
                - val_ratio: éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.1ï¼‰
                - test_ratio: æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤ 0.2ï¼‰
                - recur_times: KD-tree é€’å½’æ¬¡æ•°ï¼ˆé»˜è®¤ 1ï¼‰
                - spa_patchsize: ç©ºé—´ patch å¤§å°ï¼ˆé»˜è®¤ 4ï¼‰
            log: æ—¥å¿—æ–‡ä»¶å¯¹è±¡
        """
        self.config = config
        self.log = log
        
        # ODPS é…ç½®
        self.odps_project = config.get('odps_project', 'autonavi_traffic_report')
        self.odps_endpoint = config.get('odps_endpoint', 
                                       'http://service-corp.odps.aliyun-inc.com/api')
        self.odps_table = config.get('odps_table', 'tb_inter_spatial_method_pretrain_data')
        self.odps_meta_table = config.get('odps_meta_table', 'intersection_meta_aligned')  # é»˜è®¤ä½¿ç”¨å¯¹é½çš„å…ƒæ•°æ®è¡¨
        
        # âš ï¸ å…ƒæ•°æ®è¡¨æ˜¯å¿…é¡»çš„ï¼ˆç”¨äº KD-tree ç©ºé—´åˆ†ç»„ï¼‰
        if not self.odps_meta_table:
            raise ValueError(
                "å¿…é¡»æä¾› odps_meta_table å‚æ•°ï¼\n"
                "PatchSTG éœ€è¦èŠ‚ç‚¹ä½ç½®ä¿¡æ¯è¿›è¡Œç©ºé—´ KD-tree åˆ†ç»„ã€‚\n"
                "è¯·åœ¨é…ç½®ä¸­æ·»åŠ : odps_meta_table = 'intersection_meta_aligned'"
            )
        
        # æ•°æ®è¿‡æ»¤æ¡ä»¶
        self.adcode = config.get('adcode', None)
        self.start_date = config.get('start_date', None)
        self.end_date = config.get('end_date', None)
        self.limit = config.get('limit', None)  # å¯é€‰ï¼šé™åˆ¶æŸ¥è¯¢è¡Œæ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
        
        # è®­ç»ƒå‚æ•°
        self.input_len = config.get('input_len', 12)
        self.output_len = config.get('output_len', 12)
        self.train_ratio = config.get('train_ratio', 0.6)  # 60% è®­ç»ƒé›†
        self.val_ratio = config.get('val_ratio', 0.2)     # 20% éªŒè¯é›†
        self.test_ratio = config.get('test_ratio', 0.2)   # 20% æµ‹è¯•é›†
        
        # ç©ºé—´ patching å‚æ•°
        self.recur_times = config.get('recur_times', 1)
        self.spa_patchsize = config.get('spa_patchsize', 4)
        
        # æ•°æ®é›†
        self.trainX = None
        self.trainY = None
        self.trainXTE = None
        self.trainYTE = None
        
        self.valX = None
        self.valY = None
        self.valXTE = None
        self.valYTE = None
        
        self.testX = None
        self.testY = None
        self.testXTE = None
        self.testYTE = None
        
        # å½’ä¸€åŒ–å‚æ•°
        self.mean = None
        self.std = None
        
        # èŠ‚ç‚¹ä¿¡æ¯
        self.node_list = None  # [(nds_id, next_nds_id), ...]
        self.node_num = 0
        self.node_to_idx = {}  # {(nds_id, next_nds_id): idx}
        self.node_locations = None  # èŠ‚ç‚¹ç»çº¬åº¦ (2, num_nodes): [lat, lng]
        
        # Patch ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ç©ºé—´ patchingï¼‰
        self.ori_parts_idx = None
        self.reo_parts_idx = None
        self.reo_all_idx = None
        
        self._loaded = False
        self._odps_client = None
    
    def _init_odps_client(self):
        """åˆå§‹åŒ– ODPS å®¢æˆ·ç«¯"""
        if self._odps_client is not None:
            return
        
        access_id = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
        secret = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
        
        if not access_id or not secret:
            raise RuntimeError(
                "ç¼ºå°‘ ODPS å‡­è¯ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡:\n"
                "  ALIBABA_CLOUD_ACCESS_KEY_ID\n"
                "  ALIBABA_CLOUD_ACCESS_KEY_SECRET"
            )
        
        self._odps_client = ODPS(
            access_id,
            secret,
            project=self.odps_project,
            endpoint=self.odps_endpoint
        )
        
        if self.log:
            log_string(self.log, f'ODPS client initialized for project: {self.odps_project}')
    
    def _build_query(self):
        """æ„å»º SQL æŸ¥è¯¢è¯­å¥"""
        where_clauses = []
        
        if self.adcode:
            where_clauses.append(f"adcode = '{self.adcode}'")
        
        if self.start_date and self.end_date:
            where_clauses.append(f"ds >= '{self.start_date}' AND ds <= '{self.end_date}'")
        elif self.start_date:
            where_clauses.append(f"ds >= '{self.start_date}'")
        elif self.end_date:
            where_clauses.append(f"ds <= '{self.end_date}'")
        
        where_clause = " AND ".join(where_clauses) if where_clauses else "1=1"
        
        # æ„å»º LIMIT å­å¥
        limit_clause = f"\nLIMIT {self.limit}" if self.limit else ""
        
        query = f"""
        SELECT 
            nds_id,
            next_nds_id,
            adcode,
            ds,
            passts_time,
            flow_label,
            time_feat,
            dym_feat_feat
        FROM {self.odps_table}
        WHERE {where_clause}
        ORDER BY nds_id, next_nds_id, passts_time{limit_clause}
        """
        
        return query
    
    def _parse_time_feat(self, time_feat_str):
        """
        è§£ææ—¶é—´ç‰¹å¾å­—ç¬¦ä¸²
        
        å‚æ•°:
            time_feat_str: "week hour minute day_type day month;..." (24æ®µ)
            
        è¿”å›:
            np.ndarray: shape (24, 6)
        """
        segments = time_feat_str.split(';')
        features = []
        
        for seg in segments:
            parts = seg.strip().split()
            if len(parts) == 6:
                features.append([int(p) for p in parts])
            else:
                # å¼‚å¸¸å¤„ç†ï¼šå¡«å……é»˜è®¤å€¼
                features.append([0, 0, 0, 0, 0, 0])
        
        # ç¡®ä¿æœ‰ 24 æ®µ
        while len(features) < 24:
            features.append([0, 0, 0, 0, 0, 0])
        
        return np.array(features[:24], dtype=np.float32)
    
    def _parse_dym_feat(self, dym_feat_str):
        """
        è§£æåŠ¨æ€æµé‡ç‰¹å¾å­—ç¬¦ä¸²
        
        å‚æ•°:
            dym_feat_str: "15;8;0;12;...;3" (24ä¸ªå€¼)
            
        è¿”å›:
            np.ndarray: shape (24,)
        """
        values = dym_feat_str.split(';')
        features = []
        
        for val in values:
            try:
                features.append(float(val.strip()))
            except:
                features.append(0.0)
        
        # ç¡®ä¿æœ‰ 24 ä¸ªå€¼
        while len(features) < 24:
            features.append(0.0)
        
        return np.array(features[:24], dtype=np.float32)
    
    def load_data(self):
        """ä» ODPS åŠ è½½æ•°æ®"""
        if self._loaded:
            if self.log:
                log_string(self.log, 'Data already loaded, skipping...')
            return
        
        if self.log:
            log_string(self.log, '\n------------ Loading Data from ODPS -------------')
            log_string(self.log, f'Project: {self.odps_project}')
            log_string(self.log, f'Table: {self.odps_table}')
            log_string(self.log, f'Adcode: {self.adcode}')
            log_string(self.log, f'Date range: {self.start_date} ~ {self.end_date}')
        
        # åˆå§‹åŒ– ODPS å®¢æˆ·ç«¯
        self._init_odps_client()
        
        # æ„å»ºå¹¶æ‰§è¡ŒæŸ¥è¯¢
        query = self._build_query()
        if self.log:
            log_string(self.log, f'\nExecuting query:\n{query}\n')
        
        # æ‰§è¡ŒæŸ¥è¯¢å¹¶è½¬ä¸º DataFrame
        with self._odps_client.execute_sql(query).open_reader() as reader:
            records = [record.values for record in reader]
            columns = ['nds_id', 'next_nds_id', 'adcode', 'ds', 'passts_time', 
                      'flow_label', 'time_feat', 'dym_feat_feat']
            df = pd.DataFrame(records, columns=columns)
        
        if self.log:
            log_string(self.log, f'Loaded {len(df)} records from ODPS')
        
        if len(df) == 0:
            raise ValueError("No data loaded from ODPS. Check your filter conditions.")
        
        # æ„å»ºèŠ‚ç‚¹åˆ—è¡¨
        self._build_node_list(df)
        
        # åŠ è½½èŠ‚ç‚¹ä½ç½®ä¿¡æ¯ï¼ˆå¦‚æœæœ‰å…ƒæ•°æ®è¡¨ï¼‰
        self._load_node_locations()
        
        # å¤„ç†æ•°æ®å¹¶åˆ’åˆ†æ•°æ®é›†
        self._process_and_split_data(df)
        
        self._loaded = True
        
        if self.log:
            log_string(self.log, f'Train samples: {self.trainX.shape[0]}')
            log_string(self.log, f'Val samples: {self.valX.shape[0]}')
            log_string(self.log, f'Test samples: {self.testX.shape[0]}')
            log_string(self.log, f'Nodes: {self.node_num}')
            log_string(self.log, f'Mean: {self.mean:.4f}, Std: {self.std:.4f}')
            log_string(self.log, '------------ End -------------\n')
    
    def _build_node_list(self, df):
        """æ„å»ºèŠ‚ç‚¹åˆ—è¡¨"""
        # è·å–å”¯ä¸€çš„ (nds_id, next_nds_id) å¯¹
        node_pairs = df[['nds_id', 'next_nds_id']].drop_duplicates()
        self.node_list = [(row['nds_id'], row['next_nds_id']) 
                         for _, row in node_pairs.iterrows()]
        self.node_num = len(self.node_list)
        self.node_to_idx = {node: idx for idx, node in enumerate(self.node_list)}
        
        if self.log:
            log_string(self.log, f'Found {self.node_num} unique node pairs (road segments)')
    
    def _load_node_locations(self):
        """
        ä» ODPS å…ƒæ•°æ®è¡¨åŠ è½½è·¯å£çš„ç»çº¬åº¦ä¿¡æ¯ï¼ˆå¿…é¡»ï¼‰
        
        âš ï¸ ç»çº¬åº¦æ˜¯å¿…é¡»çš„ï¼Œç”¨äº KD-tree ç©ºé—´åˆ†ç»„
        
        æ•°æ®å…³ç³»:
        - (nds_id, next_nds_id) è¡¨ç¤ºä¸€ä¸ªè½¬å‘æµ
        - æ¯ä¸ªè½¬å‘æµå¯¹åº”ä¸€ä¸ªè·¯å£ inter_id
        - è·¯å£ inter_id æœ‰ç»çº¬åº¦åæ ‡
        
        å…ƒæ•°æ®è¡¨åº”åŒ…å«ä»¥ä¸‹å­—æ®µ:
        - nds_id: è½¬å‘å‰çš„è·¯æ®µ ID
        - next_nds_id: è½¬å‘åçš„è·¯æ®µ ID
        - inter_id: è·¯å£ ID
        - lat: è·¯å£çº¬åº¦
        - lng: è·¯å£ç»åº¦
        """
        
        if self.log:
            log_string(self.log, f'\nğŸ“ Loading node locations from: {self.odps_meta_table}')
        
        # æ„å»ºæŸ¥è¯¢ï¼šè·å–æ‰€æœ‰è½¬å‘æµå¯¹åº”çš„è·¯å£ä½ç½®
        query = f"""
        SELECT 
            nds_id,
            next_nds_id,
            inter_id,
            lat,
            lng
        FROM {self.odps_meta_table}
        WHERE 1=1
        """
        
        # å¦‚æœæœ‰ adcode è¿‡æ»¤ï¼Œä¹Ÿåº”ç”¨åˆ°å…ƒæ•°æ®è¡¨
        if self.adcode:
            query += f" AND adcode = '{self.adcode}'"
        
        if self.log:
            log_string(self.log, f'Executing meta query:\n{query}')
        
        # æ‰§è¡ŒæŸ¥è¯¢
        with self._odps_client.execute_sql(query).open_reader() as reader:
            meta_records = [record.values for record in reader]
            meta_df = pd.DataFrame(meta_records, 
                                  columns=['nds_id', 'next_nds_id', 'inter_id', 
                                         'lat', 'lng'])
        
        if len(meta_df) == 0:
            raise RuntimeError(
                f"âŒ å…ƒæ•°æ®è¡¨ {self.odps_meta_table} ä¸­æ²¡æœ‰æ‰¾åˆ°ä½ç½®æ•°æ®ï¼\n"
                f"   è¿‡æ»¤æ¡ä»¶: adcode = '{self.adcode}'\n"
                f"   PatchSTG éœ€è¦èŠ‚ç‚¹ä½ç½®è¿›è¡Œç©ºé—´åˆ†ç»„ã€‚"
            )
        
        if self.log:
            unique_intersections = meta_df['inter_id'].nunique()
            log_string(self.log, f'   Found {len(meta_df)} turn flows across {unique_intersections} intersections')
        
        # åˆ›å»ºä½ç½®æ•°ç»„ (2, num_nodes): [lat, lng]
        self.node_locations = np.zeros((2, self.node_num), dtype=np.float32)
        
        # å¡«å……æ¯ä¸ªè½¬å‘æµï¼ˆèŠ‚ç‚¹å¯¹ï¼‰çš„ä½ç½®ï¼ˆä½¿ç”¨å¯¹åº”è·¯å£çš„ä½ç½®ï¼‰
        missing_count = 0
        missing_nodes = []
        
        for idx, (nds_id, next_nds_id) in enumerate(self.node_list):
            # æŸ¥æ‰¾å¯¹åº”çš„è·¯å£ä½ç½®
            location = meta_df[
                (meta_df['nds_id'] == nds_id) & 
                (meta_df['next_nds_id'] == next_nds_id)
            ]
            
            if len(location) > 0:
                lat = location.iloc[0]['lat']
                lng = location.iloc[0]['lng']
                
                # æ£€æŸ¥ç»çº¬åº¦æ˜¯å¦æœ‰æ•ˆ
                if lat is None or lng is None or lat == 0 or lng == 0:
                    missing_count += 1
                    missing_nodes.append((nds_id, next_nds_id))
                    self.node_locations[0, idx] = 0.0
                    self.node_locations[1, idx] = 0.0
                else:
                    self.node_locations[0, idx] = lat
                    self.node_locations[1, idx] = lng
            else:
                missing_count += 1
                missing_nodes.append((nds_id, next_nds_id))
                self.node_locations[0, idx] = 0.0
                self.node_locations[1, idx] = 0.0
        
        # è®¡ç®—è¦†ç›–ç‡
        coverage = (self.node_num - missing_count) * 100.0 / self.node_num
        
        if self.log:
            log_string(self.log, f'   âœ… Loaded locations for {self.node_num - missing_count}/{self.node_num} nodes')
            log_string(self.log, f'   ğŸ“Š Coverage: {coverage:.2f}%')
        
        # âš ï¸ å¦‚æœè¦†ç›–ç‡å¤ªä½ï¼ŒæŠ¥é”™
        if coverage < 50.0:
            raise RuntimeError(
                f"âŒ ä½ç½®è¦†ç›–ç‡å¤ªä½: {coverage:.2f}%\n"
                f"   åªæœ‰ {self.node_num - missing_count}/{self.node_num} ä¸ªèŠ‚ç‚¹æœ‰æœ‰æ•ˆä½ç½®ã€‚\n"
                f"   PatchSTG éœ€è¦è‡³å°‘ 50% çš„èŠ‚ç‚¹æœ‰ä½ç½®ä¿¡æ¯æ‰èƒ½è¿›è¡Œç©ºé—´åˆ†ç»„ã€‚\n"
                f"   è¯·æ£€æŸ¥å…ƒæ•°æ®è¡¨æ˜¯å¦åŒ…å«æ‰€æœ‰èŠ‚ç‚¹çš„ä½ç½®ä¿¡æ¯ã€‚"
            )
        
        if missing_count > 0:
            if self.log:
                log_string(self.log, f'   âš ï¸  Warning: {missing_count} nodes have missing locations')
                if missing_count <= 10:
                    log_string(self.log, f'   Missing nodes: {missing_nodes[:10]}')
    
    def _process_and_split_data(self, df):
        """
        å¤„ç†æ•°æ®å¹¶åˆ’åˆ†ä¸ºè®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†
        
        âœ… æ–°å®ç°ï¼šæŒ‰æ—¶é—´çª—å£ç»„ç»‡æ•°æ®ï¼ˆå¯†é›†æ ¼å¼ï¼‰
        
        æ•°æ®ç»“æ„:
        - X: (num_samples, input_len, num_nodes, 1) - è¿‡å»çš„æµé‡å€¼
        - Y: (num_samples, output_len, num_nodes, 1) - æœªæ¥çš„æµé‡å€¼
        - XTE: (num_samples, input_len, 2) - æ—¶é—´ç‰¹å¾ (tod, dow)
        - YTE: (num_samples, output_len, 2) - æ—¶é—´ç‰¹å¾ (tod, dow)
        
        å…³é”®æ”¹è¿›ï¼šæ¯ä¸ªæ ·æœ¬åŒ…å«æ‰€æœ‰èŠ‚ç‚¹åœ¨è¿ç»­æ—¶é—´æ®µçš„æ•°æ®ï¼ˆå¯†é›†ï¼‰
        """
        if self.log:
            log_string(self.log, '\nğŸ“Š Processing data (time-series format)...')
        
        # æ­¥éª¤ 1: è½¬æ¢æ—¶é—´æˆ³æ ¼å¼å¹¶æ·»åŠ èŠ‚ç‚¹ç´¢å¼•
        if self.log:
            log_string(self.log, '   Step 1: Parsing timestamps...')
        
        df['timestamp'] = pd.to_datetime(df['passts_time'])
        df['node_idx'] = df.apply(
            lambda row: self.node_to_idx[(row['nds_id'], row['next_nds_id'])], 
            axis=1
        )
        
        # æ­¥éª¤ 2: æŒ‰åˆ†é’Ÿå¯¹é½æ—¶é—´æˆ³ï¼ˆå‘ä¸‹å–æ•´ï¼‰
        df['time_minute'] = df['timestamp'].dt.floor('1min')
        
        if self.log:
            time_range = f"{df['time_minute'].min()} ~ {df['time_minute'].max()}"
            log_string(self.log, f'   Time range: {time_range}')
        
        # æ­¥éª¤ 3: Pivot ä¸ºæ—¶é—´åºåˆ—æ ¼å¼ï¼ˆæ—¶é—´ Ã— èŠ‚ç‚¹ï¼‰
        if self.log:
            log_string(self.log, '   Step 2: Pivoting to time-series format...')
            log_string(self.log, f'   This may take a while for {len(df)} records...')
        
        # ä½¿ç”¨ pivot_table èšåˆï¼ˆå¦‚æœåŒä¸€èŠ‚ç‚¹åŒä¸€åˆ†é’Ÿæœ‰å¤šæ¡è®°å½•ï¼Œå–å¹³å‡ï¼‰
        flow_matrix = df.pivot_table(
            index='time_minute',
            columns='node_idx',
            values='flow_label',
            aggfunc='mean',  # å¦‚æœæœ‰é‡å¤ï¼Œå–å¹³å‡
            fill_value=0.0   # ç¼ºå¤±å€¼å¡«å……ä¸º 0
        )
        
        # ç¡®ä¿æ‰€æœ‰èŠ‚ç‚¹éƒ½åœ¨åˆ—ä¸­ï¼ˆæŒ‰ç´¢å¼•æ’åºï¼‰
        all_node_indices = list(range(self.node_num))
        missing_nodes = set(all_node_indices) - set(flow_matrix.columns)
        for node_idx in missing_nodes:
            flow_matrix[node_idx] = 0.0
        flow_matrix = flow_matrix[all_node_indices]  # æŒ‰èŠ‚ç‚¹ç´¢å¼•æ’åº
        
        if self.log:
            log_string(self.log, f'   âœ… Flow matrix shape: {flow_matrix.shape} (time Ã— nodes)')
            log_string(self.log, f'   Time steps: {len(flow_matrix)}')
            log_string(self.log, f'   Nodes: {len(flow_matrix.columns)}')
            
            # ç»Ÿè®¡éé›¶å€¼æ¯”ä¾‹
            non_zero_ratio = (flow_matrix.values > 0).sum() / flow_matrix.size * 100
            log_string(self.log, f'   Non-zero ratio: {non_zero_ratio:.2f}%')
        
        # æ­¥éª¤ 4: æå–æ—¶é—´ç‰¹å¾
        if self.log:
            log_string(self.log, '   Step 3: Extracting time features...')
        
        # ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹ç”Ÿæˆæ—¶é—´ç‰¹å¾
        time_features = []
        for timestamp in flow_matrix.index:
            hour = timestamp.hour
            day_of_week = timestamp.dayofweek  # 0=Monday, 6=Sunday
            
            tod = hour / 24.0      # Time of day [0, 1)
            dow = day_of_week / 7.0  # Day of week [0, 1)
            
            time_features.append([tod, dow])
        
        time_features = np.array(time_features, dtype=np.float32)
        
        # æ­¥éª¤ 5: ä½¿ç”¨æ»‘åŠ¨çª—å£ç”Ÿæˆæ ·æœ¬
        if self.log:
            log_string(self.log, '   Step 4: Generating samples with sliding window...')
        
        flow_values = flow_matrix.values  # (num_times, num_nodes)
        num_times = len(flow_values)
        num_samples = num_times - self.input_len - self.output_len + 1
        
        if num_samples <= 0:
            raise ValueError(
                f"Not enough time steps to generate samples!\n"
                f"  Time steps: {num_times}\n"
                f"  Required: input_len ({self.input_len}) + output_len ({self.output_len}) = {self.input_len + self.output_len}\n"
                f"  Please use a longer date range."
            )
        
        if self.log:
            log_string(self.log, f'   Total samples to generate: {num_samples}')
        
        # é¢„åˆ†é…æ•°ç»„
        X_data = np.zeros((num_samples, self.input_len, self.node_num, 1), dtype=np.float32)
        Y_data = np.zeros((num_samples, self.output_len, self.node_num, 1), dtype=np.float32)
        XTE_data = np.zeros((num_samples, self.input_len, 2), dtype=np.float32)
        YTE_data = np.zeros((num_samples, self.output_len, 2), dtype=np.float32)
        
        # æ»‘åŠ¨çª—å£ç”Ÿæˆæ ·æœ¬
        for i in range(num_samples):
            # è¾“å…¥ï¼šæ—¶é—´æ­¥ i åˆ° i+input_len-1
            X_data[i, :, :, 0] = flow_values[i:i+self.input_len]
            XTE_data[i] = time_features[i:i+self.input_len]
            
            # è¾“å‡ºï¼šæ—¶é—´æ­¥ i+input_len åˆ° i+input_len+output_len-1
            Y_data[i, :, :, 0] = flow_values[i+self.input_len:i+self.input_len+self.output_len]
            YTE_data[i] = time_features[i+self.input_len:i+self.input_len+self.output_len]
        
        if self.log:
            log_string(self.log, f'   âœ… Generated {num_samples} samples')
            log_string(self.log, f'   X shape: {X_data.shape}')
            log_string(self.log, f'   Y shape: {Y_data.shape}')
        
        # æ­¥éª¤ 6: éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
        if self.log:
            log_string(self.log, '   Step 5: Validating data...')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ— æ•ˆå€¼
        if np.any(np.isnan(X_data)) or np.any(np.isinf(X_data)):
            raise ValueError("X_data contains NaN or Inf values!")
        if np.any(np.isnan(Y_data)) or np.any(np.isinf(Y_data)):
            raise ValueError("Y_data contains NaN or Inf values!")
        
        # ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬ä¸­æœ‰å¤šå°‘èŠ‚ç‚¹æœ‰éé›¶å€¼
        nodes_per_sample = (X_data[:, :, :, 0] > 0).any(axis=1).sum(axis=1)
        avg_nodes = nodes_per_sample.mean()
        min_nodes = nodes_per_sample.min()
        max_nodes = nodes_per_sample.max()
        
        if self.log:
            log_string(self.log, f'   Nodes with data per sample: min={min_nodes}, max={max_nodes}, avg={avg_nodes:.1f}')
            log_string(self.log, f'   Flow value range: [{X_data.min():.2f}, {X_data.max():.2f}]')
        
        # æ­¥éª¤ 7: è®¡ç®—å½’ä¸€åŒ–å‚æ•°ï¼ˆåŸºäºè®­ç»ƒé›†ï¼‰
        num_train = int(num_samples * self.train_ratio)
        train_data = X_data[:num_train]
        
        # åªå¯¹éé›¶å€¼è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®ï¼ˆæ›´å‡†ç¡®ï¼‰
        train_nonzero = train_data[train_data > 0]
        if len(train_nonzero) > 0:
            self.mean = np.mean(train_nonzero)
            self.std = np.std(train_nonzero)
        else:
            self.mean = 0.0
            self.std = 1.0
        
        if self.std < 1e-6:
            self.std = 1.0
            if self.log:
                log_string(self.log, '   âš ï¸  Warning: std is too small, set to 1.0')
        
        if self.log:
            log_string(self.log, f'   Normalization: mean={self.mean:.4f}, std={self.std:.4f}')
        
        # æ­¥éª¤ 8: åˆ’åˆ†æ•°æ®é›†
        num_val = int(num_samples * self.val_ratio)
        num_test = int(num_samples * self.test_ratio)
        
        # ç¡®ä¿åˆ’åˆ†åè‡³å°‘æœ‰ä¸€äº›æ ·æœ¬
        if num_train < 1 or num_val < 1 or num_test < 1:
            raise ValueError(
                f"Dataset too small after split!\n"
                f"  Total samples: {num_samples}\n"
                f"  Train: {num_train}, Val: {num_val}, Test: {num_test}\n"
                f"  Please use a longer date range or adjust split ratios."
            )
        
        self.trainX = X_data[:num_train]
        self.trainY = Y_data[:num_train]
        self.trainXTE = XTE_data[:num_train]
        self.trainYTE = YTE_data[:num_train]
        
        self.valX = X_data[num_train:num_train+num_val]
        self.valY = Y_data[num_train:num_train+num_val]
        self.valXTE = XTE_data[num_train:num_train+num_val]
        self.valYTE = YTE_data[num_train:num_train+num_val]
        
        self.testX = X_data[num_train+num_val:num_train+num_val+num_test]
        self.testY = Y_data[num_train+num_val:num_train+num_val+num_test]
        self.testXTE = XTE_data[num_train+num_val:num_train+num_val+num_test]
        self.testYTE = YTE_data[num_train+num_val:num_train+num_val+num_test]
        
        if self.log:
            log_string(self.log, f'   âœ… Dataset split: Train={num_train}, Val={num_val}, Test={num_test}')
        
        # åˆ›å»ºç©ºé—´ patch ç´¢å¼•
        self._create_spatial_patches(self.trainX)
    
    def _create_spatial_patches(self, train_data):
        """
        åˆ›å»ºç©ºé—´ patch ç´¢å¼•ï¼ˆä½¿ç”¨ KD-treeï¼‰
        
        âš ï¸ å¿…é¡»æœ‰èŠ‚ç‚¹ä½ç½®ä¿¡æ¯æ‰èƒ½è¿›è¡Œç©ºé—´åˆ†ç»„
        """
        if self.log:
            log_string(self.log, '\nğŸŒ³ Creating spatial patches using KD-tree...')
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä½ç½®ä¿¡æ¯
        if self.node_locations is None:
            raise RuntimeError(
                "âŒ æ²¡æœ‰èŠ‚ç‚¹ä½ç½®ä¿¡æ¯ï¼Œæ— æ³•åˆ›å»ºç©ºé—´ patchï¼\n"
                "   è¯·ç¡®ä¿å·²åŠ è½½å…ƒæ•°æ®è¡¨ã€‚"
            )
        
        # æ£€æŸ¥ä½ç½®ä¿¡æ¯æ˜¯å¦æœ‰æ•ˆ
        valid_locations = (self.node_locations != 0).any(axis=0).sum()
        if valid_locations == 0:
            raise RuntimeError(
                "âŒ æ‰€æœ‰èŠ‚ç‚¹çš„ä½ç½®ä¿¡æ¯éƒ½æ— æ•ˆï¼ˆå…¨ä¸º0ï¼‰ï¼\n"
                "   è¯·æ£€æŸ¥å…ƒæ•°æ®è¡¨ä¸­çš„ lat, lng å­—æ®µã€‚"
            )
        
        try:
            # å¯¼å…¥åŸæœ‰çš„ patching å‡½æ•°
            from lib.utils import construct_adj, reorderData
            from sklearn.neighbors import KDTree
            
            if self.log:
                log_string(self.log, f'   Node locations shape: {self.node_locations.shape}')
                log_string(self.log, f'   Valid locations: {valid_locations}/{self.node_num}')
                log_string(self.log, f'   Lat range: [{self.node_locations[0].min():.4f}, {self.node_locations[0].max():.4f}]')
                log_string(self.log, f'   Lng range: [{self.node_locations[1].min():.4f}, {self.node_locations[1].max():.4f}]')
            
            # ä½¿ç”¨ KD-tree è¿›è¡Œç©ºé—´åˆ’åˆ†
            tree = KDTree(self.node_locations.T)  # (num_nodes, 2)
            
            # é€’å½’åˆ’åˆ†
            def recursive_split(indices, depth=0):
                if depth >= self.recur_times or len(indices) <= self.spa_patchsize:
                    return [indices]
                
                # æ‰¾åˆ°ä¸­ä½æ•°å¹¶åˆ†å‰²
                coords = self.node_locations[:, indices]
                axis = depth % 2  # 0 for lat, 1 for lng
                median_idx = len(indices) // 2
                sorted_indices = indices[np.argsort(coords[axis, :])]
                
                left = sorted_indices[:median_idx]
                right = sorted_indices[median_idx:]
                
                return recursive_split(left, depth+1) + recursive_split(right, depth+1)
            
            parts_idx = recursive_split(np.arange(self.node_num))
            
            if self.log:
                log_string(self.log, f'   âœ… Created {len(parts_idx)} spatial patches')
                patch_sizes = [len(p) for p in parts_idx]
                log_string(self.log, f'   Patch size range: [{min(patch_sizes)}, {max(patch_sizes)}]')
                log_string(self.log, f'   Average patch size: {np.mean(patch_sizes):.1f}')
            
            # æ„é€ é‚»æ¥çŸ©é˜µç”¨äºè¡¥é½
            if self.log:
                log_string(self.log, '   Constructing adjacency matrix...')
            # construct_adj æœŸæœ› (time_steps, nodes, 1)ï¼Œä½† train_data æ˜¯ (samples, time_steps, nodes, 1)
            # æˆ‘ä»¬å°†æ‰€æœ‰æ ·æœ¬æ‹¼æ¥æˆä¸€ä¸ªé•¿æ—¶é—´åºåˆ—
            train_data_concat = train_data.reshape(-1, self.node_num, 1)  # (samples*time_steps, nodes, 1)
            adj = construct_adj(train_data_concat, self.node_num)
            
            # è·å–æœ€å¤§ patch é•¿åº¦
            mxlen = max([len(p) for p in parts_idx])
            
            # é‡æ’å¹¶è¡¥é½
            if self.log:
                log_string(self.log, '   Reordering patches...')
            self.ori_parts_idx, self.reo_parts_idx, self.reo_all_idx = reorderData(
                parts_idx, mxlen, adj, self.spa_patchsize
            )
            
            if self.log:
                log_string(self.log, '   âœ… Spatial patching completed')
            
        except ImportError as e:
            raise RuntimeError(
                f"âŒ ç¼ºå°‘å¿…è¦çš„åº“: {str(e)}\n"
                f"   è¯·å®‰è£…: pip install scikit-learn"
            )
        except Exception as e:
            raise RuntimeError(
                f"âŒ ç©ºé—´ patching å¤±è´¥: {str(e)}\n"
                f"   è¿™æ˜¯åˆ›å»º KD-tree ç©ºé—´åˆ†ç»„æ—¶çš„é”™è¯¯ã€‚"
            )
    
    def get_train_data(self):
        """è·å–è®­ç»ƒæ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return self.trainX, self.trainY, self.trainXTE, self.trainYTE
    
    def get_val_data(self):
        """è·å–éªŒè¯æ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return self.valX, self.valY, self.valXTE, self.valYTE
    
    def get_test_data(self):
        """è·å–æµ‹è¯•æ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return self.testX, self.testY, self.testXTE, self.testYTE
    
    def get_normalization_params(self):
        """è·å–å½’ä¸€åŒ–å‚æ•°"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return self.mean, self.std
    
    def get_patch_indices(self):
        """è·å– patch ç´¢å¼•"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return self.ori_parts_idx, self.reo_parts_idx, self.reo_all_idx
    
    def shuffle_train_data(self, seed=None):
        """æ‰“ä¹±è®­ç»ƒæ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        
        if seed is not None:
            np.random.seed(seed)
        
        num_train = self.trainX.shape[0]
        permutation = np.random.permutation(num_train)
        
        self.trainX = self.trainX[permutation]
        self.trainY = self.trainY[permutation]
        self.trainXTE = self.trainXTE[permutation]
        
        if self.log:
            log_string(self.log, 'Training data shuffled')
    
    def normalize_data(self, data):
        """å½’ä¸€åŒ–æ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return (data - self.mean) / self.std
    
    def denormalize_data(self, data):
        """åå½’ä¸€åŒ–æ•°æ®"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        return data * self.std + self.mean
    
    def get_data_info(self):
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        if not self._loaded:
            raise RuntimeError("Data not loaded. Call load_data() first.")
        
        return {
            'train_samples': self.trainX.shape[0],
            'val_samples': self.valX.shape[0],
            'test_samples': self.testX.shape[0],
            'input_shape': self.trainX.shape[1:],
            'output_shape': self.trainY.shape[1:],
            'mean': float(self.mean),
            'std': float(self.std),
            'num_nodes': self.node_num,
            'node_list': self.node_list[:10] if len(self.node_list) > 10 else self.node_list,
            'adcode': self.adcode,
            'date_range': f'{self.start_date} ~ {self.end_date}'
        }
