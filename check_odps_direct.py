"""
ä½¿ç”¨ OdpsTableDataset ç›´æŽ¥è¯»å– ODPS è¡¨çš„æ–¹å¼
é€‚é…ä½ çš„è¡¨ç»“æž„: autonavi_traffic_report.tb_inter_spatial_method_pretrain_data
"""
import os
import numpy as np
import pandas as pd
import torch
from torch.utils.data import DataLoader
import sys

# å‡è®¾ä½ æœ‰ OdpsTableDataset å·¥å…·ç±»ï¼Œå¦‚æžœæ²¡æœ‰éœ€è¦å®‰è£…æˆ–å®žçŽ°
# è¿™é‡Œå…ˆå†™ä¸€ä¸ªæ¨¡æ‹Ÿç‰ˆæœ¬ï¼Œå®žé™…ä½¿ç”¨æ—¶æ›¿æ¢æˆçœŸå®žçš„
try:
    from utils.odps_table import OdpsTableDataset
except:
    print("âš ï¸  æœªæ‰¾åˆ° OdpsTableDatasetï¼Œéœ€è¦ä»ŽåŽŸé¡¹ç›®å¯¼å…¥æˆ–å®žçŽ°")
    # ç®€å•çš„æ¨¡æ‹Ÿå®žçŽ°ç”¨äºŽæµ‹è¯•
    class OdpsTableDataset:
        def __init__(self, table_path, slice_id=0, slice_count=1):
            """æ¨¡æ‹Ÿçš„ ODPS è¡¨è¯»å–å™¨"""
            from odps import ODPS
            
            access_id = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
            secret = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
            
            # è§£æž table_path: odps://project/tables/table_name
            parts = table_path.replace('odps://', '').split('/tables/')
            project = parts[0]
            table_name = parts[1]
            
            self.odps = ODPS(
                access_id, 
                secret, 
                project=project,
                endpoint='http://service-corp.odps.aliyun-inc.com/api'
            )
            self.table_name = table_name
            self.slice_id = slice_id
            self.slice_count = slice_count
            self._data = None
            self._load_data()
        
        def _load_data(self):
            """åŠ è½½æ•°æ®"""
            # ç®€å•æŸ¥è¯¢ï¼Œå®žé™…åº”è¯¥æ”¯æŒåˆ†ç‰‡
            query = f"""
            SELECT nds_id, next_nds_id, adcode, ds, passts_time, 
                   flow_label, time_feat, dym_feat_feat
            FROM {self.table_name}
            WHERE adcode = '650100' 
              AND ds >= '20250919' 
              AND ds <= '20250925'
            LIMIT 100
            """
            print(f"Executing query: {query}")
            
            with self.odps.execute_sql(query).open_reader() as reader:
                self._data = [list(record.values) for record in reader]
            
            print(f"Loaded {len(self._data)} records")
        
        def __len__(self):
            return len(self._data) if self._data else 0
        
        def __getitem__(self, idx):
            """è¿”å›žä¸€æ¡è®°å½•"""
            return self._data[idx]


# ========== æ•°æ®åˆ—å®šä¹‰ ==========
data_columns = [
    'nds_id',           # bigint
    'next_nds_id',      # bigint  
    'adcode',           # bigint
    'ds',               # string
    'passts_time',      # string
    'flow_label',       # bigint - é¢„æµ‹ç›®æ ‡
    'time_feat',        # string - æ—¶é—´ç‰¹å¾åºåˆ—
    'dym_feat_feat'     # string - åŽ†å²æµé‡åºåˆ—
]

# åˆ—ç´¢å¼•æ˜ å°„
COL_NDS_ID = 0
COL_NEXT_NDS_ID = 1
COL_ADCODE = 2
COL_DS = 3
COL_PASSTS_TIME = 4
COL_FLOW_LABEL = 5
COL_TIME_FEAT = 6
COL_DYM_FEAT = 7


def collate_fn(batch):
    """
    æ‰¹é‡æ•°æ®å¤„ç†å‡½æ•°
    
    è¾“å…¥: batch æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ä¸€æ¡è®°å½• [nds_id, next_nds_id, ..., time_feat, dym_feat_feat]
    è¾“å‡º: 
        - nds_ids: (batch_size,)
        - next_nds_ids: (batch_size,)
        - time_features: (batch_size, 24, 6) - 24ä¸ªæ—¶é—´æ­¥ï¼Œæ¯æ­¥6ä¸ªç‰¹å¾
        - flow_features: (batch_size, 24) - 24ä¸ªåŽ†å²æµé‡å€¼
        - labels: (batch_size,) - å½“å‰æ—¶åˆ»æµé‡æ ‡ç­¾
        - adcodes: (batch_size,)
    """
    
    nds_id_list = []
    next_nds_id_list = []
    adcode_list = []
    time_feat_list = []
    flow_feat_list = []
    label_list = []
    
    for record in batch:
        # æå–å„å­—æ®µ
        nds_id = record[COL_NDS_ID]
        next_nds_id = record[COL_NEXT_NDS_ID]
        adcode = record[COL_ADCODE]
        flow_label = record[COL_FLOW_LABEL]
        time_feat_str = record[COL_TIME_FEAT]
        dym_feat_str = record[COL_DYM_FEAT]
        
        # è§£æž time_feat: "5 17 36 0 18 8;5 17 35 0 18 8;..."
        # æ¯æ®µåŒ…å«6ä¸ªæ•°å­—: [week, hour, minute, day_type, day, month]
        try:
            time_feat_array = np.array([
                [int(x) for x in segment.split(' ')]
                for segment in time_feat_str.split(';')
            ], dtype=np.int32)  # Shape: (24, 6)
        except:
            # å¦‚æžœè§£æžå¤±è´¥ï¼Œç”¨é›¶å¡«å……
            time_feat_array = np.zeros((24, 6), dtype=np.int32)
        
        # è§£æž dym_feat_feat: "0;0;2;1;1;0;..."
        # æ¯æ®µæ˜¯ä¸€ä¸ªæµé‡å€¼
        try:
            flow_feat_array = np.array([
                float(x) for x in dym_feat_str.split(';')
            ], dtype=np.float32)  # Shape: (24,)
        except:
            flow_feat_array = np.zeros(24, dtype=np.float32)
        
        # æ·»åŠ åˆ°åˆ—è¡¨
        nds_id_list.append(nds_id)
        next_nds_id_list.append(next_nds_id)
        adcode_list.append(adcode)
        time_feat_list.append(time_feat_array)
        flow_feat_list.append(flow_feat_array)
        label_list.append(float(flow_label))
    
    # è½¬æ¢ä¸º tensor
    nds_ids = torch.tensor(nds_id_list, dtype=torch.long)
    next_nds_ids = torch.tensor(next_nds_id_list, dtype=torch.long)
    adcodes = torch.tensor(adcode_list, dtype=torch.long)
    time_features = torch.from_numpy(np.stack(time_feat_list))  # (batch, 24, 6)
    flow_features = torch.from_numpy(np.stack(flow_feat_list))  # (batch, 24)
    labels = torch.tensor(label_list, dtype=torch.float32)
    
    return {
        'nds_id': nds_ids,
        'next_nds_id': next_nds_ids,
        'adcode': adcodes,
        'time_features': time_features,
        'flow_features': flow_features,
        'labels': labels
    }


def test_direct_read():
    """æµ‹è¯•ç›´æŽ¥è¯»å– ODPS è¡¨"""
    
    print("=" * 60)
    print("æµ‹è¯•ä½¿ç”¨ OdpsTableDataset ç›´æŽ¥è¯»å– ODPS è¡¨")
    print("=" * 60)
    
    # è¡¨è·¯å¾„ï¼ˆODPSæ ¼å¼ï¼‰
    odps_table_path = "odps://autonavi_traffic_report/tables/tb_inter_spatial_method_pretrain_data"
    
    print(f"\nðŸ“Š è¡¨è·¯å¾„: {odps_table_path}")
    
    # åˆ›å»ºæ•°æ®é›†
    slice_id = 0  # å½“å‰åˆ†ç‰‡IDï¼ˆåˆ†å¸ƒå¼è®­ç»ƒæ—¶ä½¿ç”¨ï¼‰
    slice_count = 1  # æ€»åˆ†ç‰‡æ•°
    
    try:
        dataset = OdpsTableDataset(odps_table_path, slice_id, slice_count)
        print(f"âœ… æ•°æ®é›†åˆ›å»ºæˆåŠŸï¼Œå…± {len(dataset)} æ¡è®°å½•")
    except Exception as e:
        print(f"âŒ æ•°æ®é›†åˆ›å»ºå¤±è´¥: {e}")
        return
    
    # åˆ›å»º DataLoader
    data_loader = DataLoader(
        dataset,
        batch_size=4,
        shuffle=False,
        num_workers=0,
        collate_fn=collate_fn
    )
    
    print(f"\nðŸ“¦ DataLoader åˆ›å»ºæˆåŠŸï¼Œbatch_size=4")
    
    # æµ‹è¯•è¯»å–ä¸€ä¸ª batch
    print("\n" + "=" * 60)
    print("æµ‹è¯•è¯»å–ç¬¬ä¸€ä¸ª batch")
    print("=" * 60)
    
    for batch_idx, batch in enumerate(data_loader):
        print(f"\nðŸ”¹ Batch {batch_idx + 1}:")
        print(f"  nds_id shape: {batch['nds_id'].shape}")
        print(f"  nds_id values: {batch['nds_id']}")
        print(f"  next_nds_id shape: {batch['next_nds_id'].shape}")
        print(f"  adcode shape: {batch['adcode'].shape}")
        print(f"  adcode values: {batch['adcode']}")
        print(f"  time_features shape: {batch['time_features'].shape}")  # (batch, 24, 6)
        print(f"  time_features[0, 0, :]: {batch['time_features'][0, 0, :]}")  # ç¬¬ä¸€ä¸ªæ ·æœ¬ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥
        print(f"  flow_features shape: {batch['flow_features'].shape}")  # (batch, 24)
        print(f"  flow_features[0, :]: {batch['flow_features'][0, :]}")  # ç¬¬ä¸€ä¸ªæ ·æœ¬çš„åŽ†å²æµé‡
        print(f"  labels shape: {batch['labels'].shape}")
        print(f"  labels values: {batch['labels']}")
        
        # åªè¯»å–ç¬¬ä¸€ä¸ª batch
        break
    
    print("\n" + "=" * 60)
    print("âœ… æµ‹è¯•å®Œæˆï¼")
    print("=" * 60)


def test_with_meta_table():
    """æµ‹è¯•å…³è”å…ƒæ•°æ®è¡¨èŽ·å–ç»çº¬åº¦"""
    
    print("\n" + "=" * 60)
    print("æµ‹è¯•å…³è”å…ƒæ•°æ®è¡¨èŽ·å–è·¯å£ç»çº¬åº¦")
    print("=" * 60)
    
    from odps import ODPS
    
    access_id = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID')
    secret = os.getenv('ALIBABA_CLOUD_ACCESS_KEY_SECRET')
    
    odps = ODPS(
        access_id, 
        secret, 
        project='autonavi_traffic_report',
        endpoint='http://service-corp.odps.aliyun-inc.com/api'
    )
    
    # æŸ¥è¯¢ï¼šå…³è”ä¸»è¡¨å’Œå…ƒæ•°æ®è¡¨
    query = """
    SELECT 
        f.nds_id,
        f.next_nds_id,
        f.adcode,
        f.flow_label,
        m.inter_id,
        m.lat,
        m.lng
    FROM autonavi_traffic_report.tb_inter_spatial_method_pretrain_data f
    LEFT JOIN autonavi_traffic_report.intersection_meta_1 m
        ON f.nds_id = m.nds_id 
        AND f.next_nds_id = m.next_nds_id
        AND f.adcode = m.adcode
    WHERE f.adcode = '650100' 
      AND f.ds = '20250919'
    LIMIT 10
    """
    
    print(f"\næ‰§è¡Œå…³è”æŸ¥è¯¢:")
    print(query)
    
    try:
        with odps.execute_sql(query).open_reader() as reader:
            results = []
            for record in reader:
                results.append(record.values)
            
            print(f"\nâœ… æŸ¥è¯¢æˆåŠŸï¼Œè¿”å›ž {len(results)} æ¡è®°å½•\n")
            
            # æ˜¾ç¤ºç»“æžœ
            for i, row in enumerate(results[:5]):
                nds_id, next_nds_id, adcode, flow_label, inter_id, lat, lng = row
                print(f"è®°å½• {i+1}:")
                print(f"  è½¬å‘æµ: ({nds_id}, {next_nds_id})")
                print(f"  è·¯å£: {inter_id}")
                print(f"  ä½ç½®: ({lat}, {lng})")
                print(f"  æµé‡: {flow_label}")
                print()
            
            # ç»Ÿè®¡æœ‰å¤šå°‘è®°å½•æœ‰ç»çº¬åº¦
            has_location = sum(1 for row in results if row[5] is not None)
            print(f"ðŸ“Š ç»Ÿè®¡:")
            print(f"  æ€»è®°å½•æ•°: {len(results)}")
            print(f"  æœ‰ç»çº¬åº¦: {has_location}")
            print(f"  è¦†ç›–çŽ‡: {has_location/len(results)*100:.1f}%")
            
    except Exception as e:
        print(f"âŒ æŸ¥è¯¢å¤±è´¥: {e}")


if __name__ == '__main__':
    # æµ‹è¯•1: ç›´æŽ¥è¯»å–ä¸»è¡¨
    test_direct_read()
    
    # æµ‹è¯•2: å…³è”å…ƒæ•°æ®è¡¨
    test_with_meta_table()
