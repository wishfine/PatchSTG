"""
æµ‹è¯•æ–°çš„æ•°æ®åŠ è½½é€»è¾‘
éªŒè¯æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®ï¼ˆå¯†é›†æ ¼å¼ï¼‰
"""
import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import numpy as np

# è®¾ç½®ç¯å¢ƒå˜é‡ï¼ˆå¦‚æœè¿˜æ²¡è®¾ç½®ï¼‰
if not os.getenv('ALIBABA_CLOUD_ACCESS_KEY_ID'):
    print("âš ï¸  è¯·å…ˆè®¾ç½® ODPS ç¯å¢ƒå˜é‡")
    sys.exit(1)

from lib.odps_data_loader import ODPSDataLoader
from lib.utils import log_string

print("=" * 80)
print("ğŸ§ª æµ‹è¯•æ–°çš„æ•°æ®åŠ è½½é€»è¾‘ï¼ˆå¯†é›†æ ¼å¼ï¼‰")
print("=" * 80)

# åˆ›å»ºæ—¥å¿—
log = open('./log/test_data_loader.log', 'w')

# é…ç½®
config = {
    'odps_project': 'autonavi_traffic_report',
    'odps_endpoint': 'http://service-corp.odps.aliyun-inc.com/api',
    'odps_table': 'tb_inter_spatial_method_pretrain_data',
    'odps_meta_table': 'intersection_meta_aligned',
    'adcode': '650100',
    'start_date': '20250919',
    'end_date': '20250919',  # åªæµ‹è¯•1å¤©æ•°æ®
    'input_len': 12,
    'output_len': 12,
    'train_ratio': 0.6,
    'val_ratio': 0.2,
    'test_ratio': 0.2,
    'recur_times': 1,
    'spa_patchsize': 4
}

print("\nğŸ“‹ é…ç½®:")
print(f"  åŸå¸‚: {config['adcode']} (ä¹Œé²æœ¨é½)")
print(f"  æ—¥æœŸ: {config['start_date']} ~ {config['end_date']}")
print(f"  è¾“å…¥é•¿åº¦: {config['input_len']}")
print(f"  è¾“å‡ºé•¿åº¦: {config['output_len']}")
print(f"  æ•°æ®åˆ’åˆ†: {config['train_ratio']:.0%} / {config['val_ratio']:.0%} / {config['test_ratio']:.0%}")

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
print("\n" + "=" * 80)
print("ğŸ“Š åŠ è½½æ•°æ®...")
print("=" * 80)

try:
    loader = ODPSDataLoader(config, log)
    loader.load_data()
    
    print("\nâœ… æ•°æ®åŠ è½½æˆåŠŸï¼")
    
except Exception as e:
    print(f"\nâŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
    import traceback
    traceback.print_exc()
    log.close()
    sys.exit(1)

# è·å–æ•°æ®
trainX, trainY, trainXTE, trainYTE = loader.get_train_data()
valX, valY, valXTE, valYTE = loader.get_val_data()
testX, testY, testXTE, testYTE = loader.get_test_data()
mean, std = loader.get_normalization_params()

# éªŒè¯æ•°æ®å½¢çŠ¶
print("\n" + "=" * 80)
print("ğŸ“ æ•°æ®å½¢çŠ¶éªŒè¯")
print("=" * 80)

print(f"\nè®­ç»ƒé›†:")
print(f"  X shape: {trainX.shape}")
print(f"  Y shape: {trainY.shape}")
print(f"  TE shape: {trainXTE.shape}")
print(f"  æœŸæœ›: (samples, 12, {loader.node_num}, 1)")

print(f"\néªŒè¯é›†:")
print(f"  X shape: {valX.shape}")
print(f"  Y shape: {valY.shape}")

print(f"\næµ‹è¯•é›†:")
print(f"  X shape: {testX.shape}")
print(f"  Y shape: {testY.shape}")

# éªŒè¯æ˜¯å¦æ˜¯å¯†é›†æ ¼å¼ï¼ˆå…³é”®æµ‹è¯•ï¼ï¼‰
print("\n" + "=" * 80)
print("ğŸ” å¯†é›†æ€§éªŒè¯ï¼ˆå…³é”®ï¼‰")
print("=" * 80)

# æ£€æŸ¥æ¯ä¸ªæ ·æœ¬æœ‰å¤šå°‘èŠ‚ç‚¹æœ‰éé›¶å€¼
def check_density(X, name):
    print(f"\n{name}:")
    # å¯¹äºæ¯ä¸ªæ ·æœ¬ï¼Œç»Ÿè®¡æœ‰å¤šå°‘èŠ‚ç‚¹åœ¨ä»»æ„æ—¶é—´æ­¥æœ‰éé›¶å€¼
    nodes_with_data = []
    for i in range(min(5, len(X))):  # æ£€æŸ¥å‰5ä¸ªæ ·æœ¬
        # X[i]: (T, N, 1)
        has_data = (X[i, :, :, 0] > 0).any(axis=0)  # (N,) - æ¯ä¸ªèŠ‚ç‚¹æ˜¯å¦æœ‰æ•°æ®
        num_nodes = has_data.sum()
        nodes_with_data.append(num_nodes)
        
        if i < 3:  # è¯¦ç»†æ˜¾ç¤ºå‰3ä¸ªæ ·æœ¬
            print(f"  æ ·æœ¬ {i}: {num_nodes}/{loader.node_num} ä¸ªèŠ‚ç‚¹æœ‰æ•°æ® ({num_nodes/loader.node_num*100:.1f}%)")
            
            # æ˜¾ç¤ºæ ·æœ¬çš„æµé‡èŒƒå›´
            sample_data = X[i, :, :, 0]
            non_zero = sample_data[sample_data > 0]
            if len(non_zero) > 0:
                print(f"         æµé‡èŒƒå›´: [{non_zero.min():.2f}, {non_zero.max():.2f}]")
    
    avg_nodes = np.mean(nodes_with_data)
    min_nodes = np.min(nodes_with_data)
    max_nodes = np.max(nodes_with_data)
    
    print(f"  å¹³å‡: {avg_nodes:.1f} ä¸ªèŠ‚ç‚¹æœ‰æ•°æ® ({avg_nodes/loader.node_num*100:.1f}%)")
    print(f"  èŒƒå›´: [{min_nodes}, {max_nodes}]")
    
    # åˆ¤æ–­æ˜¯å¦å¯†é›†
    if avg_nodes / loader.node_num > 0.5:
        print(f"  âœ… æ•°æ®æ˜¯å¯†é›†çš„ï¼ˆ> 50% èŠ‚ç‚¹æœ‰æ•°æ®ï¼‰")
        return True
    elif avg_nodes / loader.node_num > 0.1:
        print(f"  âš ï¸  æ•°æ®ç¨€ç–åº¦ä¸­ç­‰ï¼ˆ10-50% èŠ‚ç‚¹æœ‰æ•°æ®ï¼‰")
        return False
    else:
        print(f"  âŒ æ•°æ®æ˜¯ç¨€ç–çš„ï¼ˆ< 10% èŠ‚ç‚¹æœ‰æ•°æ®ï¼‰")
        return False

is_dense_train = check_density(trainX, "è®­ç»ƒé›†")
is_dense_val = check_density(valX, "éªŒè¯é›†")

# éªŒè¯æ•°æ®å€¼
print("\n" + "=" * 80)
print("ğŸ“Š æ•°æ®ç»Ÿè®¡")
print("=" * 80)

print(f"\næµé‡å€¼ç»Ÿè®¡:")
print(f"  è®­ç»ƒé›†èŒƒå›´: [{trainX.min():.2f}, {trainX.max():.2f}]")
print(f"  éªŒè¯é›†èŒƒå›´: [{valX.min():.2f}, {valX.max():.2f}]")
print(f"  æµ‹è¯•é›†èŒƒå›´: [{testX.min():.2f}, {testX.max():.2f}]")

print(f"\nå½’ä¸€åŒ–å‚æ•°:")
print(f"  Mean: {mean:.4f}")
print(f"  Std: {std:.4f}")

# éé›¶å€¼æ¯”ä¾‹
train_nonzero_ratio = (trainX > 0).sum() / trainX.size * 100
val_nonzero_ratio = (valX > 0).sum() / valX.size * 100

print(f"\néé›¶å€¼æ¯”ä¾‹:")
print(f"  è®­ç»ƒé›†: {train_nonzero_ratio:.2f}%")
print(f"  éªŒè¯é›†: {val_nonzero_ratio:.2f}%")

# æ—¶é—´ç‰¹å¾éªŒè¯
print("\n" + "=" * 80)
print("â° æ—¶é—´ç‰¹å¾éªŒè¯")
print("=" * 80)

print(f"\nå‰3ä¸ªæ ·æœ¬çš„æ—¶é—´ç‰¹å¾:")
for i in range(min(3, len(trainXTE))):
    te = trainXTE[i]
    print(f"\næ ·æœ¬ {i}:")
    print(f"  è¾“å…¥æ—¶é—´æ­¥ 0: tod={te[0,0]:.3f}, dow={te[0,1]:.3f}")
    print(f"  è¾“å…¥æ—¶é—´æ­¥ 11: tod={te[11,0]:.3f}, dow={te[11,1]:.3f}")

# ç©ºé—´ patch éªŒè¯
print("\n" + "=" * 80)
print("ğŸŒ³ ç©ºé—´ Patch éªŒè¯")
print("=" * 80)

ori_parts, reo_parts, reo_all = loader.get_patch_indices()

if ori_parts is not None:
    print(f"  Patch æ•°é‡: {len(ori_parts)}")
    patch_sizes = [len(p) for p in ori_parts]
    print(f"  Patch å¤§å°èŒƒå›´: [{min(patch_sizes)}, {max(patch_sizes)}]")
    print(f"  å¹³å‡ Patch å¤§å°: {np.mean(patch_sizes):.1f}")
    print(f"  âœ… ç©ºé—´ patch åˆ›å»ºæˆåŠŸ")
else:
    print(f"  âŒ æœªåˆ›å»ºç©ºé—´ patch")

# æœ€ç»ˆåˆ¤å®š
print("\n" + "=" * 80)
print("âœ… æµ‹è¯•ç»“æœ")
print("=" * 80)

success = True
issues = []

# 1. å½¢çŠ¶æ£€æŸ¥
if trainX.shape[1:] != (12, loader.node_num, 1):
    success = False
    issues.append(f"âŒ è®­ç»ƒé›†å½¢çŠ¶é”™è¯¯: {trainX.shape}")
else:
    print("âœ… æ•°æ®å½¢çŠ¶æ­£ç¡®")

# 2. å¯†é›†æ€§æ£€æŸ¥
if not is_dense_train:
    success = False
    issues.append("âŒ æ•°æ®ä»ç„¶æ˜¯ç¨€ç–çš„ï¼ˆæ¯ä¸ªæ ·æœ¬åªæœ‰å°‘æ•°èŠ‚ç‚¹æœ‰å€¼ï¼‰")
else:
    print("âœ… æ•°æ®æ˜¯å¯†é›†çš„ï¼ˆæ¯ä¸ªæ ·æœ¬åŒ…å«å¤§éƒ¨åˆ†èŠ‚ç‚¹ï¼‰")

# 3. æ•°æ®æœ‰æ•ˆæ€§æ£€æŸ¥
if np.any(np.isnan(trainX)) or np.any(np.isinf(trainX)):
    success = False
    issues.append("âŒ æ•°æ®åŒ…å« NaN æˆ– Inf")
else:
    print("âœ… æ•°æ®æ—  NaN/Inf")

# 4. æ ·æœ¬æ•°é‡åˆç†æ€§
expected_samples = 1440 - 12 - 12 + 1  # 1å¤©1440åˆ†é’Ÿ
actual_total = len(trainX) + len(valX) + len(testX)
if abs(actual_total - expected_samples) > 10:
    print(f"âš ï¸  æ ·æœ¬æ•°é‡å¼‚å¸¸: æœŸæœ›çº¦{expected_samples}ï¼Œå®é™…{actual_total}")
else:
    print(f"âœ… æ ·æœ¬æ•°é‡åˆç†: {actual_total}")

# 5. Patch æ£€æŸ¥
if ori_parts is None:
    success = False
    issues.append("âŒ æœªåˆ›å»ºç©ºé—´ patch")
else:
    print("âœ… ç©ºé—´ patch å·²åˆ›å»º")

print("\n" + "=" * 80)
if success:
    print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ•°æ®æ ¼å¼æ­£ç¡®ï¼Œå¯ä»¥å¼€å§‹è®­ç»ƒã€‚")
else:
    print("âš ï¸  å­˜åœ¨é—®é¢˜:")
    for issue in issues:
        print(f"  {issue}")
print("=" * 80)

log.close()

print(f"\nè¯¦ç»†æ—¥å¿—å·²ä¿å­˜åˆ°: ./log/test_data_loader.log")
