# ODPS æµå¼æ•°æ®åŠ è½½æ”¹è¿›

## ğŸ“Œ æ”¹è¿›æ¦‚è¿°

å°†æ•°æ®åŠ è½½å™¨ä»**ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®**æ”¹ä¸º**æµå¼è¯»å–**ï¼Œé¿å…å¤§è§„æ¨¡æ•°æ®ï¼ˆå¦‚ä¸€ä¸ªæœˆæ•°æ®ï¼‰å¯¼è‡´çš„å†…å­˜æº¢å‡ºé—®é¢˜ã€‚

## ğŸ”„ æ”¹è¿›å‰åå¯¹æ¯”

### âŒ æ”¹è¿›å‰ï¼ˆå­˜åœ¨é—®é¢˜ï¼‰
```python
# ä¸€æ¬¡æ€§åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
with self._odps_client.execute_sql(query).open_reader() as reader:
    records = [record.values for record in reader]  # âš ï¸ æ‰€æœ‰æ•°æ®ä¸€æ¬¡æ€§è¯»å…¥
    df = pd.DataFrame(records, columns=columns)
```

**é—®é¢˜**ï¼š
- âŒ å†…å­˜å ç”¨å¤§ï¼šæ‰€æœ‰æ•°æ®å¿…é¡»ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜
- âŒ æ— æ³•å¤„ç†å¤§è§„æ¨¡æ•°æ®ï¼šä¸€ä¸ªæœˆæ•°æ®å¯èƒ½å¯¼è‡´ OOMï¼ˆå†…å­˜æº¢å‡ºï¼‰
- âŒ å¯åŠ¨æ—¶é—´é•¿ï¼šå¿…é¡»ç­‰æ‰€æœ‰æ•°æ®åŠ è½½å®Œæ‰èƒ½å¼€å§‹å¤„ç†

---

### âœ… æ”¹è¿›åï¼ˆæµå¼è¯»å–ï¼‰
```python
# åˆ†æ‰¹æµå¼è¯»å–
chunk_size = 100000  # æ¯æ‰¹ 10 ä¸‡æ¡è®°å½•

with self._odps_client.execute_sql(query).open_reader() as reader:
    chunk_records = []
    
    for record in reader:  # âœ… é€æ¡è¿­ä»£ï¼Œä¸ä¸€æ¬¡æ€§åŠ è½½
        chunk_records.append(record.values)
        
        if len(chunk_records) >= chunk_size:
            self._process_chunk(chunk_records, time_series_dict)
            chunk_records = []  # é‡Šæ”¾å†…å­˜
```

**ä¼˜åŠ¿**ï¼š
- âœ… å†…å­˜å ç”¨å°ï¼šåªä¿ç•™å½“å‰æ‰¹æ¬¡æ•°æ®ï¼ˆ10ä¸‡æ¡ï¼‰
- âœ… æ”¯æŒå¤§è§„æ¨¡æ•°æ®ï¼šå¯å¤„ç† TB çº§åˆ«æ•°æ®
- âœ… å®æ—¶åé¦ˆï¼šæ¯æ‰¹å¤„ç†åæ˜¾ç¤ºè¿›åº¦
- âœ… å®¹é”™æ€§å¼ºï¼šå¤„ç†å¤±è´¥åªå½±å“å½“å‰æ‰¹æ¬¡

---

## ğŸš€ æ ¸å¿ƒæ”¹è¿›ç‚¹

### 1. **åˆ†ç¦»èŠ‚ç‚¹åˆ—è¡¨æŸ¥è¯¢**
æ”¹è¿›å‰æ··åˆåœ¨ä¸€èµ·ï¼Œæ”¹è¿›åå…ˆå•ç‹¬æŸ¥è¯¢èŠ‚ç‚¹åˆ—è¡¨ï¼ˆæ•°æ®é‡å°ï¼‰ï¼š

```python
def _load_node_list_from_odps(self):
    """ä½¿ç”¨ DISTINCT æŸ¥è¯¢å”¯ä¸€èŠ‚ç‚¹å¯¹ï¼ˆè½»é‡çº§æŸ¥è¯¢ï¼‰"""
    query = """
    SELECT DISTINCT nds_id, next_nds_id
    FROM {table}
    WHERE {conditions}
    """
    # åªè¿”å›èŠ‚ç‚¹åˆ—è¡¨ï¼Œæ•°æ®é‡å°ï¼Œä¸ä¼š OOM
```

### 2. **åˆ†æ‰¹æµå¼å¤„ç†**
æ¯æ‰¹å¤„ç† 10 ä¸‡æ¡è®°å½•ï¼Œç´¯ç§¯åˆ°æ—¶é—´åºåˆ—å­—å…¸ï¼š

```python
def _stream_and_process_data(self):
    """æµå¼è¯»å–å¹¶åˆ†æ‰¹å¤„ç†"""
    chunk_size = 100000
    time_series_dict = {}  # ç´¯ç§¯æ—¶é—´åºåˆ—æ•°æ®
    
    with reader as r:
        for record in r:
            # ç´¯ç§¯åˆ°æ‰¹æ¬¡
            if batch_full:
                self._process_chunk(batch, time_series_dict)
                # é‡Šæ”¾å†…å­˜
```

### 3. **å¢é‡ç´¯ç§¯æ—¶é—´åºåˆ—**
ä¸å†ä½¿ç”¨ Pandas pivot_tableï¼ˆå†…å­˜å¯†é›†ï¼‰ï¼Œæ”¹ç”¨å­—å…¸ç´¯ç§¯ï¼š

```python
def _process_chunk(self, records, time_series_dict):
    """å¤„ç†ä¸€æ‰¹è®°å½•ï¼Œç´¯ç§¯åˆ°å­—å…¸"""
    for record in records:
        time_key = record['time_minute']
        node_idx = record['node_idx']
        flow_value = record['flow_label']
        
        if time_key not in time_series_dict:
            time_series_dict[time_key] = {}
        
        time_series_dict[time_key][node_idx] = flow_value
```

### 4. **æœ€ç»ˆè½¬æ¢ä¸º NumPy æ•°ç»„**
æ‰€æœ‰æ•°æ®å¤„ç†å®Œåï¼Œä¸€æ¬¡æ€§è½¬æ¢ä¸ºè®­ç»ƒæ•°æ®æ ¼å¼ï¼š

```python
def _build_time_series_from_dict(self, time_series_dict):
    """ä»å­—å…¸æ„å»ºæœ€ç»ˆçš„è®­ç»ƒæ•°æ®"""
    # æ’åºæ—¶é—´ç‚¹
    sorted_times = sorted(time_series_dict.keys())
    
    # æ„å»ºæµé‡çŸ©é˜µ
    flow_matrix = np.zeros((num_times, num_nodes))
    for t_idx, time_key in enumerate(sorted_times):
        for node_idx, flow_value in time_series_dict[time_key].items():
            flow_matrix[t_idx, node_idx] = flow_value
    
    # æ»‘åŠ¨çª—å£ç”Ÿæˆæ ·æœ¬...
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | æ”¹è¿›å‰ | æ”¹è¿›å |
|------|--------|--------|
| **å†…å­˜å ç”¨** | æ‰€æœ‰æ•°æ®ï¼ˆå¯èƒ½æ•°GBï¼‰ | 10ä¸‡æ¡è®°å½•ï¼ˆçº¦å‡ åMBï¼‰ |
| **å¯åŠ¨æ—¶é—´** | ç­‰å¾…æ‰€æœ‰æ•°æ®åŠ è½½ | ç«‹å³å¼€å§‹å¤„ç† |
| **æ”¯æŒæ•°æ®é‡** | å—å†…å­˜é™åˆ¶ï¼ˆå‡ ç™¾ä¸‡æ¡ï¼‰ | æ— é™åˆ¶ï¼ˆTBçº§ï¼‰ |
| **è¿›åº¦å¯è§æ€§** | æ— ï¼ˆé»‘ç®±ç­‰å¾…ï¼‰ | å®æ—¶æ˜¾ç¤ºæ‰¹æ¬¡è¿›åº¦ |
| **å®¹é”™æ€§** | å¤±è´¥éœ€é‡æ–°åŠ è½½æ‰€æœ‰æ•°æ® | å¤±è´¥åªé‡è¯•å½“å‰æ‰¹æ¬¡ |

---

## ğŸ¯ ä½¿ç”¨æ–¹å¼

**å®Œå…¨é€æ˜ï¼Œæ— éœ€ä¿®æ”¹è°ƒç”¨ä»£ç ï¼**

```python
# åŸæœ‰ä»£ç æ— éœ€ä¿®æ”¹
loader = ODPSDataLoader(config, log)
loader.load_data()  # å†…éƒ¨è‡ªåŠ¨ä½¿ç”¨æµå¼è¯»å–

trainX, trainY, trainXTE, trainYTE = loader.get_train_data()
```

---

## âš™ï¸ é…ç½®å‚æ•°

### æ‰¹æ¬¡å¤§å°ï¼ˆå¯è°ƒæ•´ï¼‰
åœ¨ `_stream_and_process_data()` æ–¹æ³•ä¸­ï¼š

```python
chunk_size = 100000  # é»˜è®¤ 10 ä¸‡æ¡/æ‰¹
```

**è°ƒä¼˜å»ºè®®**ï¼š
- å†…å­˜å……è¶³ï¼šå¢å¤§åˆ° `500000`ï¼ˆ50ä¸‡ï¼‰æå‡é€Ÿåº¦
- å†…å­˜ç´§å¼ ï¼šå‡å°åˆ° `50000`ï¼ˆ5ä¸‡ï¼‰é™ä½å†…å­˜å ç”¨
- æç«¯æƒ…å†µï¼š`10000`ï¼ˆ1ä¸‡ï¼‰æœ€å°å†…å­˜å ç”¨

---

## ğŸ“ æ—¥å¿—è¾“å‡ºç¤ºä¾‹

```
------------ Loading Data from ODPS (Streaming) -------------
Project: autonavi_traffic_report
Table: tb_inter_spatial_method_pretrain_data
Adcode: 110000
Date range: 20250701 ~ 20250731

Step 1: Loading node list...
   Querying unique nodes...
   âœ… Found 15946 unique node pairs

Step 2: Loading node locations...
   ğŸ“ Loading node locations from: intersection_meta_aligned
   Found 15946 turn flows across 3500 intersections
   âœ… Loaded locations for 15946/15946 nodes
   ğŸ“Š Coverage: 100.00%

Step 3: Streaming data from ODPS...
   Executing streaming query...
   Reading data in chunks of 100000 records...
   Processed 100000 records...
   Processed 200000 records...
   Processed 300000 records...
   ...
   âœ… Total records processed: 1234567
   Unique time steps: 44640
   Converting to time series format...
   Time range: 2025-07-01 00:00:00 ~ 2025-07-31 23:59:00
   Time steps: 44640
   Flow matrix shape: (44640, 15946)
   Non-zero ratio: 45.23%
   Generating samples with sliding window...
   âœ… Generated 44617 samples
   Normalization: mean=5.3421, std=2.1234
   âœ… Dataset split: Train=26770, Val=8923, Test=8924

âœ… Data loading completed!
Train samples: 26770
Val samples: 8923
Test samples: 8924
Nodes: 15946
Mean: 5.3421, Std: 2.1234
------------ End -------------
```

---

## ğŸ”§ æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: å†…å­˜ä»ç„¶ä¸è¶³
**ç—‡çŠ¶**ï¼šå³ä½¿ä½¿ç”¨æµå¼è¯»å–ï¼Œä»ç„¶ OOM

**åŸå› **ï¼šæ—¶é—´åºåˆ—å­—å…¸ç´¯ç§¯æ•°æ®è¿‡å¤š

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. å‡å°‘æ—¥æœŸèŒƒå›´ï¼ˆå¦‚åªè®­ç»ƒ 7 å¤©æ•°æ®ï¼‰
2. ä½¿ç”¨ `limit` å‚æ•°å¿«é€Ÿæµ‹è¯•
3. è€ƒè™‘é¢„å¤„ç†ï¼šå°†æ•°æ®ä¿å­˜ä¸º `.npy` æ–‡ä»¶

```python
# æµ‹è¯•æ—¶é™åˆ¶æ•°æ®é‡
config['limit'] = 100000  # åªåŠ è½½ 10 ä¸‡æ¡è®°å½•
```

### é—®é¢˜ 2: å¤„ç†é€Ÿåº¦æ…¢
**ç—‡çŠ¶**ï¼šæ¯æ‰¹å¤„ç†è€—æ—¶è¾ƒé•¿

**åŸå› **ï¼šæ‰¹æ¬¡å¤ªå°ï¼Œæ‰¹æ¬¡æ•°é‡è¿‡å¤š

**è§£å†³æ–¹æ¡ˆ**ï¼šå¢å¤§æ‰¹æ¬¡å¤§å°
```python
chunk_size = 500000  # å¢å¤§åˆ° 50 ä¸‡æ¡/æ‰¹
```

### é—®é¢˜ 3: æ•°æ®ä¸å®Œæ•´
**ç—‡çŠ¶**ï¼šæœ€åå‡ æ¡æ•°æ®æœªå¤„ç†

**åŸå› **ï¼šæœ€åä¸€æ‰¹ä¸è¶³ `chunk_size` æœªè¢«å¤„ç†

**è§£å†³æ–¹æ¡ˆ**ï¼šå·²åœ¨ä»£ç ä¸­å¤„ç†ï¼ˆå¤„ç†æœ€åä¸€æ‰¹å‰©ä½™æ•°æ®ï¼‰
```python
# å¤„ç†æœ€åä¸€æ‰¹
if chunk_records:
    self._process_chunk(chunk_records, time_series_dict)
```

---

## ğŸŒŸ æ ¸å¿ƒä¼˜åŠ¿æ€»ç»“

1. **å†…å­˜å‹å¥½**ï¼šä» GB çº§é™è‡³ MB çº§
2. **å¯æ‰©å±•æ€§å¼º**ï¼šæ”¯æŒæœˆçº§ã€å¹´çº§æ•°æ®è®­ç»ƒ
3. **é€æ˜å‡çº§**ï¼šæ— éœ€ä¿®æ”¹ç°æœ‰è°ƒç”¨ä»£ç 
4. **å®æ—¶åé¦ˆ**ï¼šæ¯æ‰¹å¤„ç†åæ˜¾ç¤ºè¿›åº¦
5. **ç”Ÿäº§çº§è´¨é‡**ï¼šå®Œæ•´çš„æ—¥å¿—å’Œé”™è¯¯å¤„ç†

---

## ğŸ“š å‚è€ƒå®ç°

æœ¬æ”¹è¿›å‚è€ƒäº† SFT é¡¹ç›®çš„æµå¼è¯»å–å®ç°ï¼ˆ`SFT_scale_unclean_fsd.ipynb`ï¼‰ï¼Œå¹¶ç»“åˆ PatchSTG çš„å…·ä½“éœ€æ±‚è¿›è¡Œäº†ä¼˜åŒ–ã€‚

æ ¸å¿ƒæ€æƒ³ï¼š
- ä½¿ç”¨ ODPS Table Iterator è€Œä¸æ˜¯ä¸€æ¬¡æ€§è¯»å–
- åˆ†æ‰¹å¤„ç†ç´¯ç§¯åˆ°å­—å…¸
- æœ€ç»ˆä¸€æ¬¡æ€§è½¬æ¢ä¸ºè®­ç»ƒæ•°æ®æ ¼å¼

---

**ç‰ˆæœ¬**: 1.0  
**æ—¥æœŸ**: 2025å¹´11æœˆ12æ—¥  
**ä½œè€…**: PatchSTG Team
