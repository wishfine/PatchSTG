"""
PatchSTG æ•°æ®é¢„å¤„ç†è„šæœ¬ - æ–¹æ¡ˆ 2ï¼ˆç»ˆæç‰ˆï¼‰

åŠŸèƒ½ï¼š
1. ä» ODPS åŸå§‹è¡¨è¯»å–æµé‡æ•°æ®ï¼ˆæµå¼ï¼‰
2. è½¬æ¢ä¸ºæ—¶é—´åºåˆ—æ ¼å¼
3. åˆ›å»ºç©ºé—´ patchesï¼ˆKD-tree åˆ†ç»„ï¼‰
4. ç”Ÿæˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•æ ·æœ¬
5. ä¿å­˜åˆ° ODPS æ ·æœ¬è¡¨ï¼ˆä¾›è®­ç»ƒç›´æ¥è¯»å–ï¼‰

è¿è¡Œä¸€æ¬¡ï¼Œè®­ç»ƒæ— é™æ¬¡ï¼
"""

import os
import sys
import argparse
import numpy as np
import pandas as pd
from datetime import datetime
from tqdm import tqdm

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from lib.odps_data_loader import ODPSDataLoader
from lib.utils import log_string


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description='é¢„å¤„ç† PatchSTG è®­ç»ƒæ ·æœ¬')
    
    # ODPS é…ç½®
    parser.add_argument('--odps_project', type=str, required=True,
                        help='ODPS é¡¹ç›®å')
    parser.add_argument('--odps_endpoint', type=str, required=True,
                        help='ODPS endpoint')
    parser.add_argument('--odps_table', type=str, required=True,
                        help='åŸå§‹æ•°æ®è¡¨å')
    parser.add_argument('--odps_meta_table', type=str, required=True,
                        help='èŠ‚ç‚¹å…ƒæ•°æ®è¡¨å')
    parser.add_argument('--output_table', type=str, required=True,
                        help='è¾“å‡ºæ ·æœ¬è¡¨åï¼ˆä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»ºï¼‰')
    
    # æ•°æ®è¿‡æ»¤
    parser.add_argument('--adcode', type=str, default='110000',
                        help='åŸå¸‚ä»£ç ï¼ˆé»˜è®¤ï¼š110000 åŒ—äº¬ï¼‰')
    parser.add_argument('--start_date', type=str, required=True,
                        help='å¼€å§‹æ—¥æœŸ YYYYMMDD')
    parser.add_argument('--end_date', type=str, required=True,
                        help='ç»“æŸæ—¥æœŸ YYYYMMDD')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--input_len', type=int, default=12,
                        help='è¾“å…¥åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ï¼š12ï¼‰')
    parser.add_argument('--output_len', type=int, default=12,
                        help='è¾“å‡ºåºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ï¼š12ï¼‰')
    parser.add_argument('--train_ratio', type=float, default=0.6,
                        help='è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆé»˜è®¤ï¼š0.6ï¼‰')
    parser.add_argument('--val_ratio', type=float, default=0.2,
                        help='éªŒè¯é›†æ¯”ä¾‹ï¼ˆé»˜è®¤ï¼š0.2ï¼‰')
    parser.add_argument('--test_ratio', type=float, default=0.2,
                        help='æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆé»˜è®¤ï¼š0.2ï¼‰')
    
    # ç©ºé—´å‚æ•°
    parser.add_argument('--recur_times', type=int, default=1,
                        help='KD-tree é€’å½’æ¬¡æ•°ï¼ˆé»˜è®¤ï¼š1ï¼‰')
    parser.add_argument('--spa_patchsize', type=int, default=4,
                        help='ç©ºé—´ patch å¤§å°ï¼ˆé»˜è®¤ï¼š4ï¼‰')
    
    # å…¶ä»–
    parser.add_argument('--batch_size', type=int, default=10000,
                        help='å†™å…¥ ODPS çš„æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ï¼š10000ï¼‰')
    
    return parser.parse_args()


def save_samples_to_odps(odps_client, table_name, samples_df, batch_size=10000):
    """
    å°†æ ·æœ¬ä¿å­˜åˆ° ODPS è¡¨
    
    å‚æ•°:
        odps_client: ODPS å®¢æˆ·ç«¯
        table_name: è¡¨å
        samples_df: æ ·æœ¬ DataFrame
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    from odps import TableSchema
    from odps.models import Column
    
    print(f"\nğŸ“ å‡†å¤‡å†™å…¥ ODPS è¡¨: {table_name}")
    
    # å®šä¹‰è¡¨ç»“æ„
    schema = TableSchema([
        Column('sample_id', 'string'),      # æ ·æœ¬ID
        Column('split', 'string'),          # train/val/test
        Column('X', 'string'),              # è¾“å…¥åºåˆ— (åºåˆ—åŒ–)
        Column('Y', 'string'),              # è¾“å‡ºåºåˆ— (åºåˆ—åŒ–)
        Column('TE_X', 'string'),           # è¾“å…¥æ—¶é—´ç‰¹å¾ (åºåˆ—åŒ–)
        Column('TE_Y', 'string'),           # è¾“å‡ºæ—¶é—´ç‰¹å¾ (åºåˆ—åŒ–)
        Column('node_indices', 'string'),   # èŠ‚ç‚¹ç´¢å¼•åˆ—è¡¨
        Column('timestamp', 'string'),      # æ ·æœ¬æ—¶é—´æˆ³
    ])
    
    # å¦‚æœè¡¨ä¸å­˜åœ¨ï¼Œåˆ›å»º
    if not odps_client.exist_table(table_name):
        print(f"   åˆ›å»ºæ–°è¡¨: {table_name}")
        odps_client.create_table(table_name, schema)
    
    table = odps_client.get_table(table_name)
    
    # åˆ†æ‰¹å†™å…¥
    total_samples = len(samples_df)
    num_batches = (total_samples + batch_size - 1) // batch_size
    
    print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
    print(f"   æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"   æ€»æ‰¹æ¬¡æ•°: {num_batches}")
    print(f"\nå¼€å§‹å†™å…¥...")
    
    with table.open_writer() as writer:
        for batch_idx in tqdm(range(num_batches), desc="å†™å…¥è¿›åº¦"):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, total_samples)
            batch_df = samples_df.iloc[start_idx:end_idx]
            
            # è½¬æ¢ä¸ºè®°å½•åˆ—è¡¨
            records = []
            for _, row in batch_df.iterrows():
                records.append([
                    row['sample_id'],
                    row['split'],
                    row['X'],
                    row['Y'],
                    row['TE_X'],
                    row['TE_Y'],
                    row['node_indices'],
                    row['timestamp'],
                ])
            
            writer.write(records)
    
    print(f"\nâœ… æˆåŠŸå†™å…¥ {total_samples} ä¸ªæ ·æœ¬åˆ° {table_name}")


def serialize_array(arr):
    """å°† numpy æ•°ç»„åºåˆ—åŒ–ä¸ºå­—ç¬¦ä¸²"""
    return ','.join(arr.flatten().astype(str))


def create_samples_dataframe(trainX, trainY, trainXTE, trainYTE, split='train'):
    """
    å°† numpy æ•°ç»„è½¬æ¢ä¸º DataFrame
    
    å‚æ•°:
        trainX: (N, T_in, num_nodes, C)
        trainY: (N, T_out, num_nodes, C)
        trainXTE: (N, T_in, 2)
        trainYTE: (N, T_out, 2)
        split: 'train' / 'val' / 'test'
    
    è¿”å›:
        DataFrame with columns: sample_id, split, X, Y, TE_X, TE_Y, node_indices, timestamp
    """
    num_samples = trainX.shape[0]
    num_nodes = trainX.shape[2]
    
    print(f"\nğŸ”„ è½¬æ¢ {split} æ•°æ®ä¸º DataFrame...")
    print(f"   æ ·æœ¬æ•°: {num_samples}")
    print(f"   èŠ‚ç‚¹æ•°: {num_nodes}")
    
    samples = []
    for i in tqdm(range(num_samples), desc=f"å¤„ç† {split}"):
        sample_id = f"{split}_{i}"
        
        # åºåˆ—åŒ–æ•°ç»„
        X_str = serialize_array(trainX[i])        # (T_in, num_nodes, C)
        Y_str = serialize_array(trainY[i])        # (T_out, num_nodes, C)
        TE_X_str = serialize_array(trainXTE[i])   # (T_in, 2)
        TE_Y_str = serialize_array(trainYTE[i])   # (T_out, 2)
        
        # èŠ‚ç‚¹ç´¢å¼•
        node_indices_str = ','.join([str(j) for j in range(num_nodes)])
        
        # æ—¶é—´æˆ³ï¼ˆä»æ—¶é—´ç‰¹å¾æå–ï¼‰
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        samples.append({
            'sample_id': sample_id,
            'split': split,
            'X': X_str,
            'Y': Y_str,
            'TE_X': TE_X_str,
            'TE_Y': TE_Y_str,
            'node_indices': node_indices_str,
            'timestamp': timestamp,
        })
    
    return pd.DataFrame(samples)


def main():
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    print("=" * 80)
    print("ğŸš€ PatchSTG æ•°æ®é¢„å¤„ç† - æ–¹æ¡ˆ 2")
    print("=" * 80)
    print(f"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # é…ç½®
    config = {
        'odps_project': args.odps_project,
        'odps_endpoint': args.odps_endpoint,
        'odps_table': args.odps_table,
        'odps_meta_table': args.odps_meta_table,
        'adcode': args.adcode,
        'start_date': args.start_date,
        'end_date': args.end_date,
        'input_len': args.input_len,
        'output_len': args.output_len,
        'train_ratio': args.train_ratio,
        'val_ratio': args.val_ratio,
        'test_ratio': args.test_ratio,
        'recur_times': args.recur_times,
        'spa_patchsize': args.spa_patchsize,
    }
    
    print("ğŸ“‹ é…ç½®:")
    for k, v in config.items():
        print(f"  {k}: {v}")
    print()
    
    # æ­¥éª¤ 1: ä½¿ç”¨ ODPSDataLoader åŠ è½½å’Œå¤„ç†æ•°æ®
    print("=" * 80)
    print("æ­¥éª¤ 1: åŠ è½½å’Œå¤„ç†åŸå§‹æ•°æ®")
    print("=" * 80)
    
    log_file = open(f'preprocess_{args.adcode}_{args.start_date}_{args.end_date}.log', 'w')
    data_loader = ODPSDataLoader(config, log_file)
    
    # åŠ è½½æ•°æ®ï¼ˆä¼šè‡ªåŠ¨è¿›è¡Œæµå¼å¤„ç†ã€æ—¶é—´åºåˆ—è½¬æ¢ã€ç©ºé—´åˆ†ç»„ã€æ ·æœ¬ç”Ÿæˆï¼‰
    data_loader.load_data()
    
    # è·å–å¤„ç†åçš„æ•°æ®
    trainX, trainY, trainXTE, trainYTE = data_loader.get_train_data()
    valX, valY, valXTE, valYTE = data_loader.get_val_data()
    testX, testY, testXTE, testYTE = data_loader.get_test_data()
    
    print("\nâœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"  è®­ç»ƒé›†: {trainX.shape[0]} æ ·æœ¬")
    print(f"  éªŒè¯é›†: {valX.shape[0]} æ ·æœ¬")
    print(f"  æµ‹è¯•é›†: {testX.shape[0]} æ ·æœ¬")
    
    # æ­¥éª¤ 2: è½¬æ¢ä¸º DataFrame
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 2: è½¬æ¢ä¸ºæ ·æœ¬æ ¼å¼")
    print("=" * 80)
    
    train_df = create_samples_dataframe(trainX, trainY, trainXTE, trainYTE, 'train')
    val_df = create_samples_dataframe(valX, valY, valXTE, valYTE, 'val')
    test_df = create_samples_dataframe(testX, testY, testXTE, testYTE, 'test')
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    all_samples_df = pd.concat([train_df, val_df, test_df], ignore_index=True)
    
    print(f"\nâœ… æ ·æœ¬è½¬æ¢å®Œæˆ")
    print(f"  æ€»æ ·æœ¬æ•°: {len(all_samples_df)}")
    print(f"  è®­ç»ƒ: {len(train_df)} ({len(train_df)/len(all_samples_df)*100:.1f}%)")
    print(f"  éªŒè¯: {len(val_df)} ({len(val_df)/len(all_samples_df)*100:.1f}%)")
    print(f"  æµ‹è¯•: {len(test_df)} ({len(test_df)/len(all_samples_df)*100:.1f}%)")
    
    # æ­¥éª¤ 3: ä¿å­˜åˆ° ODPS
    print("\n" + "=" * 80)
    print("æ­¥éª¤ 3: ä¿å­˜åˆ° ODPS æ ·æœ¬è¡¨")
    print("=" * 80)
    
    odps_client = data_loader._odps_client
    save_samples_to_odps(
        odps_client,
        args.output_table,
        all_samples_df,
        batch_size=args.batch_size
    )
    
    # ä¿å­˜å…ƒæ•°æ®
    metadata = {
        'source_table': args.odps_table,
        'output_table': args.output_table,
        'adcode': args.adcode,
        'date_range': f"{args.start_date}~{args.end_date}",
        'total_samples': len(all_samples_df),
        'train_samples': len(train_df),
        'val_samples': len(val_df),
        'test_samples': len(test_df),
        'node_num': trainX.shape[2],
        'input_len': args.input_len,
        'output_len': args.output_len,
        'mean': float(data_loader.mean),
        'std': float(data_loader.std),
        'created_at': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    }
    
    print("\nğŸ“Š é¢„å¤„ç†å®Œæˆï¼å…ƒæ•°æ®:")
    for k, v in metadata.items():
        print(f"  {k}: {v}")
    
    # ä¿å­˜å…ƒæ•°æ®åˆ°æœ¬åœ°
    import json
    metadata_file = f"metadata_{args.output_table}.json"
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
    print(f"\nğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")
    
    log_file.close()
    
    print("\n" + "=" * 80)
    print("ğŸ‰ é¢„å¤„ç†å®Œæˆï¼")
    print("=" * 80)
    print(f"è¾“å‡ºè¡¨: {args.output_table}")
    print(f"æ€»æ ·æœ¬: {len(all_samples_df)}")
    print(f"ç»“æŸæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("\nä¸‹ä¸€æ­¥ï¼šä½¿ç”¨ train_from_samples.py è¿›è¡Œè®­ç»ƒ")
    print("=" * 80)


if __name__ == '__main__':
    main()
