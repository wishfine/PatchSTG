{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f0e1e7f",
   "metadata": {},
   "source": [
    "# PatchSTG åˆ†æ‰¹åŠ è½½è®­ç»ƒ Notebook (æ–¹æ¡ˆ 3)\n",
    "\n",
    "**ğŸ’¡ æ··åˆæ¨¡å¼ï¼šåˆ†å¤©/åˆ†æ‰¹åŠ è½½æ•°æ®ï¼Œé™ä½å†…å­˜å ç”¨**\n",
    "\n",
    "## æ ¸å¿ƒæ”¹è¿›\n",
    "\n",
    "- âœ… **ä¸ä¸€æ¬¡æ€§åŠ è½½å…¨éƒ¨æ•°æ®**ï¼ˆ7å¤© â†’ æ¯æ¬¡2å¤©ï¼‰\n",
    "- âœ… **å†…å­˜å ç”¨å¤§å¹…é™ä½**ï¼ˆ50GB+ â†’ ~10GBï¼‰\n",
    "- âœ… **ä»£ç æ”¹åŠ¨æœ€å°**ï¼ˆ~50è¡Œï¼‰\n",
    "- âœ… **ä¿ç•™æ‰€æœ‰ç°æœ‰é€»è¾‘**\n",
    "\n",
    "## é…ç½®ä¿¡æ¯\n",
    "\n",
    "- åŸå¸‚ï¼šåŒ—äº¬ (adcode = 110000)\n",
    "- æ€»æ—¥æœŸï¼š2025-09-19 ~ 2025-09-25 (7å¤©)\n",
    "- **æ¯æ¬¡åŠ è½½ï¼š2å¤©æ•°æ®**\n",
    "- æ•°æ®åˆ’åˆ†ï¼š60% / 20% / 20%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e343a5f",
   "metadata": {},
   "source": [
    "## 1. ç¯å¢ƒå‡†å¤‡å’Œå¯¼å…¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c47a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import configparser\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# æ·»åŠ é¡¹ç›®è·¯å¾„\n",
    "sys.path.insert(0, os.path.dirname(os.path.abspath('.')))\n",
    "\n",
    "from models.model import PatchSTG\n",
    "from lib.utils import log_string, _compute_loss, metric\n",
    "from lib.odps_data_loader import ODPSDataLoader\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸš€ PatchSTG åˆ†æ‰¹åŠ è½½è®­ç»ƒ (æ–¹æ¡ˆ 3)\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"å¼€å§‹æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"PyTorch ç‰ˆæœ¬: {torch.__version__}\")\n",
    "print(f\"CUDA å¯ç”¨: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA è®¾å¤‡: {torch.cuda.get_device_name(0)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af85920",
   "metadata": {},
   "source": [
    "## 2. é…ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dc6023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¯»å–é…ç½®æ–‡ä»¶\n",
    "config_file = 'config/ODPS.conf'\n",
    "config = configparser.ConfigParser()\n",
    "config.read(config_file)\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "print(f\"ğŸ“… è®­ç»ƒæ—¶é—´æˆ³: {timestamp}\")\n",
    "\n",
    "# è®­ç»ƒå‚æ•°\n",
    "args = {\n",
    "    # è®­ç»ƒé…ç½®\n",
    "    'cuda': config['train']['cuda'],\n",
    "    'seed': int(config['train']['seed']),\n",
    "    'batch_size': int(config['train']['batch_size']),\n",
    "    'max_epoch': int(config['train']['max_epoch']),\n",
    "    'learning_rate': float(config['train']['learning_rate']),\n",
    "    'weight_decay': float(config['train']['weight_decay']),\n",
    "    \n",
    "    # ODPS æ•°æ®é…ç½®\n",
    "    'odps_project': config['data']['odps_project'],\n",
    "    'odps_endpoint': config['data']['odps_endpoint'],\n",
    "    'odps_table': config['data']['odps_table'],\n",
    "    'odps_meta_table': config['data']['odps_meta_table'],\n",
    "    'adcode': config['data']['adcode'],\n",
    "    'start_date': config['data']['start_date'],  # æ€»æ—¥æœŸèŒƒå›´\n",
    "    'end_date': config['data']['end_date'],\n",
    "    'input_len': int(config['data']['input_len']),\n",
    "    'output_len': int(config['data']['output_len']),\n",
    "    'train_ratio': float(config['data']['train_ratio']),\n",
    "    'val_ratio': float(config['data']['val_ratio']),\n",
    "    'test_ratio': float(config['data']['test_ratio']),\n",
    "    \n",
    "    # æ¨¡å‹å‚æ•°\n",
    "    'layers': int(config['param']['layers']),\n",
    "    'tem_patchsize': int(config['param']['tps']),\n",
    "    'tem_patchnum': int(config['param']['tpn']),\n",
    "    'factors': int(config['param']['factors']),\n",
    "    'recur_times': int(config['param']['recur']),\n",
    "    'spa_patchsize': int(config['param']['sps']),\n",
    "    'spa_patchnum': int(config['param']['spn']),\n",
    "    'tod': int(config['param']['tod']),\n",
    "    'dow': int(config['param']['dow']),\n",
    "    'input_dims': int(config['param']['id']),\n",
    "    'node_dims': int(config['param']['nd']),\n",
    "    'tod_dims': int(config['param']['td']),\n",
    "    'dow_dims': int(config['param']['dd']),\n",
    "    \n",
    "    # ğŸ†• åˆ†æ‰¹åŠ è½½é…ç½®\n",
    "    'days_per_batch': 2,  # æ¯æ¬¡åŠ è½½2å¤©æ•°æ®\n",
    "    \n",
    "    # æ–‡ä»¶è·¯å¾„\n",
    "    'model_file': f'./saved_models/odps_beijing_incremental_{timestamp}.pth',\n",
    "    'log_file': f'./log/odps_beijing_incremental_{timestamp}',\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“‹ è®­ç»ƒé…ç½®:\")\n",
    "print(f\"  åŸå¸‚ä»£ç : {args['adcode']} (åŒ—äº¬)\")\n",
    "print(f\"  æ€»æ—¥æœŸèŒƒå›´: {args['start_date']} ~ {args['end_date']}\")\n",
    "print(f\"  ğŸ†• æ¯æ‰¹åŠ è½½: {args['days_per_batch']} å¤©æ•°æ®\")\n",
    "print(f\"  Batch Size: {args['batch_size']}\")\n",
    "print(f\"  Max Epochs: {args['max_epoch']}\")\n",
    "print()\n",
    "\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "os.makedirs('log', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ee2802",
   "metadata": {},
   "source": [
    "## 3. ç”Ÿæˆæ—¥æœŸæ‰¹æ¬¡\n",
    "\n",
    "å°†æ€»æ—¥æœŸèŒƒå›´åˆ†å‰²æˆå¤šä¸ªå°æ‰¹æ¬¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4179ecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_date_batches(start_date_str, end_date_str, days_per_batch):\n",
    "    \"\"\"\n",
    "    ç”Ÿæˆæ—¥æœŸæ‰¹æ¬¡\n",
    "    \n",
    "    å‚æ•°:\n",
    "        start_date_str: '20250919'\n",
    "        end_date_str: '20250925'\n",
    "        days_per_batch: 2\n",
    "    \n",
    "    è¿”å›:\n",
    "        [('20250919', '20250920'), ('20250921', '20250922'), ...]\n",
    "    \"\"\"\n",
    "    start_date = datetime.strptime(start_date_str, '%Y%m%d')\n",
    "    end_date = datetime.strptime(end_date_str, '%Y%m%d')\n",
    "    \n",
    "    batches = []\n",
    "    current_date = start_date\n",
    "    \n",
    "    while current_date <= end_date:\n",
    "        batch_end = min(current_date + timedelta(days=days_per_batch - 1), end_date)\n",
    "        batches.append((\n",
    "            current_date.strftime('%Y%m%d'),\n",
    "            batch_end.strftime('%Y%m%d')\n",
    "        ))\n",
    "        current_date = batch_end + timedelta(days=1)\n",
    "    \n",
    "    return batches\n",
    "\n",
    "# ç”Ÿæˆæ—¥æœŸæ‰¹æ¬¡\n",
    "date_batches = generate_date_batches(\n",
    "    args['start_date'],\n",
    "    args['end_date'],\n",
    "    args['days_per_batch']\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“… å°† {args['start_date']} ~ {args['end_date']} åˆ†æˆ {len(date_batches)} æ‰¹:\")\n",
    "for i, (batch_start, batch_end) in enumerate(date_batches, 1):\n",
    "    print(f\"   æ‰¹æ¬¡ {i}: {batch_start} ~ {batch_end}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0763496",
   "metadata": {},
   "source": [
    "## 4. è®¾ç½®éšæœºç§å­å’Œæ—¥å¿—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701bb052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®¾ç½®éšæœºç§å­\n",
    "if args['seed'] is not None:\n",
    "    random.seed(args['seed'])\n",
    "    np.random.seed(args['seed'])\n",
    "    torch.manual_seed(args['seed'])\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(args['seed'])\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    print(f\"âœ… éšæœºç§å­å·²è®¾ç½®: {args['seed']}\")\n",
    "\n",
    "# åˆå§‹åŒ–æ—¥å¿—\n",
    "log = open(args['log_file'], 'w')\n",
    "log_string(log, '=' * 80)\n",
    "log_string(log, 'PatchSTG åˆ†æ‰¹åŠ è½½è®­ç»ƒ (æ–¹æ¡ˆ 3)')\n",
    "log_string(log, '=' * 80)\n",
    "log_string(log, f\"è®­ç»ƒæ—¶é—´æˆ³: {timestamp}\")\n",
    "log_string(log, f\"æ¯æ‰¹åŠ è½½: {args['days_per_batch']} å¤©\")\n",
    "log_string(log, f\"æ€»æ‰¹æ¬¡æ•°: {len(date_batches)}\")\n",
    "log_string(log, '')\n",
    "\n",
    "print(f\"âœ… æ—¥å¿—åˆå§‹åŒ–: {args['log_file']}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7a89c8",
   "metadata": {},
   "source": [
    "## 5. åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä¸ç«‹å³åŠ è½½æ•°æ®ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3842b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ğŸ“Š åˆ›å»ºæ•°æ®åŠ è½½å™¨...\")\n",
    "\n",
    "# åˆ›å»ºæ•°æ®åŠ è½½å™¨ï¼ˆä¸ä¼šç«‹å³åŠ è½½æ•°æ®ï¼‰\n",
    "data_loader = ODPSDataLoader(args, log)\n",
    "\n",
    "# ğŸ”‘ å…³é”®ï¼šé¦–æ¬¡åŠ è½½ç¬¬ä¸€æ‰¹æ•°æ®ï¼Œè·å–èŠ‚ç‚¹ä¿¡æ¯å’Œå½’ä¸€åŒ–å‚æ•°\n",
    "print(f\"\\nğŸ”„ é¦–æ¬¡åŠ è½½ç¬¬ä¸€æ‰¹æ•°æ®: {date_batches[0][0]} ~ {date_batches[0][1]}\")\n",
    "data_loader.load_data_for_date_range(date_batches[0][0], date_batches[0][1])\n",
    "\n",
    "# è·å–å¿…è¦çš„å…¨å±€ä¿¡æ¯ï¼ˆè¿™äº›ä¿¡æ¯åœ¨æ•´ä¸ªè®­ç»ƒä¸­ä¸å˜ï¼‰\n",
    "node_num = data_loader.node_num\n",
    "mean, std = data_loader.get_normalization_params()\n",
    "ori_parts_idx, reo_parts_idx, reo_all_idx = data_loader.get_patch_indices()\n",
    "\n",
    "print(f\"\\nâœ… æ•°æ®åŠ è½½å™¨åˆå§‹åŒ–å®Œæˆ\")\n",
    "print(f\"  èŠ‚ç‚¹æ•°: {node_num}\")\n",
    "print(f\"  å½’ä¸€åŒ–: mean={mean:.4f}, std={std:.4f}\")\n",
    "print(f\"  é¦–æ‰¹æ ·æœ¬: {data_loader.get_train_data()[0].shape[0]}\")\n",
    "print()\n",
    "\n",
    "# æ¸…ç©ºé¦–æ‰¹æ•°æ®ï¼Œå‡†å¤‡è®­ç»ƒ\n",
    "data_loader.clear_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a3795f",
   "metadata": {},
   "source": [
    "## 6. æ„å»ºæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5381931",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_string(log, '\\n------------ æ„å»ºæ¨¡å‹ -------------')\n",
    "print(\"ğŸ—ï¸  æ„å»º PatchSTG æ¨¡å‹...\\n\")\n",
    "\n",
    "device = torch.device(f\"cuda:{args['cuda']}\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"è®¾å¤‡: {device}\")\n",
    "\n",
    "model = PatchSTG(\n",
    "    args['output_len'], \n",
    "    args['tem_patchsize'], \n",
    "    args['tem_patchnum'],\n",
    "    node_num,\n",
    "    args['spa_patchsize'], \n",
    "    args['spa_patchnum'],\n",
    "    args['tod'], \n",
    "    args['dow'],\n",
    "    args['layers'], \n",
    "    args['factors'],\n",
    "    args['input_dims'], \n",
    "    args['node_dims'], \n",
    "    args['tod_dims'], \n",
    "    args['dow_dims'],\n",
    "    ori_parts_idx, \n",
    "    reo_parts_idx, \n",
    "    reo_all_idx\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"æ¨¡å‹å‚æ•°: {total_params:,}\")\n",
    "\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=args['learning_rate'],\n",
    "    weight_decay=args['weight_decay']\n",
    ")\n",
    "\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(\n",
    "    optimizer,\n",
    "    milestones=[1, 35, 40],\n",
    "    gamma=0.5,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… æ¨¡å‹æ„å»ºæˆåŠŸ\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009303c0",
   "metadata": {},
   "source": [
    "## 7. å®šä¹‰éªŒè¯å‡½æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da889484",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, valX, valY, valXTE, mean, std, device, batch_size, log):\n",
    "    \"\"\"\n",
    "    éªŒè¯å‡½æ•°\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    num_val = valX.shape[0]\n",
    "    pred = []\n",
    "    label = []\n",
    "\n",
    "    num_batch = math.ceil(num_val / batch_size)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(num_batch):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(num_val, (batch_idx + 1) * batch_size)\n",
    "\n",
    "            X = valX[start_idx:end_idx]\n",
    "            Y = valY[start_idx:end_idx]\n",
    "            TE = torch.from_numpy(valXTE[start_idx:end_idx]).to(device)\n",
    "            NormX = torch.from_numpy((X - mean) / std).float().to(device)\n",
    "\n",
    "            y_hat = model(NormX, TE)\n",
    "            pred.append(y_hat.cpu().numpy() * std + mean)\n",
    "            label.append(Y)\n",
    "    \n",
    "    pred = np.concatenate(pred, axis=0)\n",
    "    label = np.concatenate(label, axis=0)\n",
    "\n",
    "    maes = []\n",
    "    rmses = []\n",
    "    mapes = []\n",
    "\n",
    "    for i in range(pred.shape[1]):\n",
    "        mae, rmse, mape = metric(pred[:, i, :], label[:, i, :])\n",
    "        maes.append(mae)\n",
    "        rmses.append(rmse)\n",
    "        mapes.append(mape)\n",
    "    \n",
    "    mae, rmse, mape = metric(pred, label)\n",
    "    maes.append(mae)\n",
    "    rmses.append(rmse)\n",
    "    mapes.append(mape)\n",
    "    \n",
    "    return np.stack(maes, 0), np.stack(rmses, 0), np.stack(mapes, 0)\n",
    "\n",
    "print(\"âœ… éªŒè¯å‡½æ•°å®šä¹‰å®Œæˆ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f33e45",
   "metadata": {},
   "source": [
    "## 8. å¼€å§‹åˆ†æ‰¹åŠ è½½è®­ç»ƒ ğŸš€\n",
    "\n",
    "**æ ¸å¿ƒæ”¹è¿›ï¼šæ¯ä¸ª epoch å¾ªç¯éå†æ‰€æœ‰æ—¥æœŸæ‰¹æ¬¡**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaf5886",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_string(log, '\\n' + '=' * 80)\n",
    "log_string(log, 'å¼€å§‹åˆ†æ‰¹åŠ è½½è®­ç»ƒ')\n",
    "log_string(log, '=' * 80)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸš€ å¼€å§‹åˆ†æ‰¹åŠ è½½è®­ç»ƒ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"æ€» Epoch æ•°: {args['max_epoch']}\")\n",
    "print(f\"æ¯ Epoch æ•°æ®æ‰¹æ¬¡: {len(date_batches)}\")\n",
    "print(f\"Batch Size: {args['batch_size']}\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "min_val_loss = float('inf')\n",
    "best_epoch = 0\n",
    "train_losses = []\n",
    "val_maes = []\n",
    "\n",
    "try:\n",
    "    for epoch in range(1, args['max_epoch'] + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_batch_count = 0\n",
    "        \n",
    "        # ğŸ†• æ¯ä¸ª epoch éå†æ‰€æœ‰æ—¥æœŸæ‰¹æ¬¡\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Epoch {epoch}/{args['max_epoch']}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        \n",
    "        # æ‰“ä¹±æ—¥æœŸæ‰¹æ¬¡é¡ºåºï¼ˆå¢åŠ éšæœºæ€§ï¼‰\n",
    "        shuffled_batches = date_batches.copy()\n",
    "        random.shuffle(shuffled_batches)\n",
    "        \n",
    "        for batch_idx, (batch_start, batch_end) in enumerate(shuffled_batches, 1):\n",
    "            print(f\"\\nğŸ“¦ æ‰¹æ¬¡ {batch_idx}/{len(shuffled_batches)}: {batch_start} ~ {batch_end}\")\n",
    "            \n",
    "            # ğŸ”‘ åŠ è½½è¿™æ‰¹æ—¥æœŸçš„æ•°æ®\n",
    "            data_loader.load_data_for_date_range(batch_start, batch_end)\n",
    "            trainX, trainY, trainXTE, trainYTE = data_loader.get_train_data()\n",
    "            \n",
    "            print(f\"   åŠ è½½äº† {trainX.shape[0]} ä¸ªè®­ç»ƒæ ·æœ¬\")\n",
    "            \n",
    "            # è®­ç»ƒè¿™æ‰¹æ•°æ®\n",
    "            num_train = trainX.shape[0]\n",
    "            num_batch = math.ceil(num_train / args['batch_size'])\n",
    "            \n",
    "            pbar = tqdm(\n",
    "                range(num_batch),\n",
    "                desc=f\"   Epoch {epoch} - Batch {batch_idx}\",\n",
    "                leave=False\n",
    "            )\n",
    "            \n",
    "            for sub_batch_idx in pbar:\n",
    "                start_idx = sub_batch_idx * args['batch_size']\n",
    "                end_idx = min(num_train, (sub_batch_idx + 1) * args['batch_size'])\n",
    "\n",
    "                X = trainX[start_idx:end_idx]\n",
    "                Y = trainY[start_idx:end_idx]\n",
    "                TE = torch.from_numpy(trainXTE[start_idx:end_idx]).to(device)\n",
    "                NormX = torch.from_numpy((X - mean) / std).float().to(device)\n",
    "                Y_tensor = torch.from_numpy(Y).float().to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                y_hat = model(NormX, TE)\n",
    "                loss = _compute_loss(Y_tensor, y_hat * std + mean)\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    "                optimizer.step()\n",
    "                \n",
    "                epoch_train_loss += loss.cpu().item()\n",
    "                epoch_batch_count += 1\n",
    "                \n",
    "                pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
    "            \n",
    "            # ğŸ—‘ï¸ é‡Šæ”¾å½“å‰æ‰¹æ¬¡çš„æ•°æ®\n",
    "            data_loader.clear_data()\n",
    "            del trainX, trainY, trainXTE, trainYTE\n",
    "            \n",
    "            # ğŸ§¹ æ¸…ç† GPU ç¼“å­˜\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            print(f\"   âœ… æ‰¹æ¬¡ {batch_idx} å®Œæˆï¼Œå†…å­˜å·²é‡Šæ”¾\")\n",
    "        \n",
    "        # Epoch ç»“æŸï¼Œè®¡ç®—å¹³å‡æŸå¤±\n",
    "        avg_train_loss = epoch_train_loss / epoch_batch_count\n",
    "        train_losses.append(avg_train_loss)\n",
    "        \n",
    "        # éªŒè¯ï¼ˆä½¿ç”¨æœ€åä¸€ä¸ªæ—¥æœŸæ‰¹æ¬¡çš„éªŒè¯é›†ï¼‰\n",
    "        print(f\"\\nğŸ“Š Epoch {epoch} è®­ç»ƒå®Œæˆï¼Œå¼€å§‹éªŒè¯...\")\n",
    "        data_loader.load_data_for_date_range(date_batches[-1][0], date_batches[-1][1])\n",
    "        valX, valY, valXTE, valYTE = data_loader.get_val_data()\n",
    "        \n",
    "        log_string(log, f'\\nEpoch {epoch}/{args[\"max_epoch\"]}:')\n",
    "        log_string(log, f'  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}')\n",
    "        log_string(log, f'  ç”¨æ—¶: {time.time() - epoch_start_time:.1f}s')\n",
    "        \n",
    "        maes, rmses, mapes = validate(\n",
    "            model, valX, valY, valXTE, mean, std, device, args['batch_size'], log\n",
    "        )\n",
    "        \n",
    "        val_mae = maes[-1]\n",
    "        val_maes.append(val_mae)\n",
    "        \n",
    "        print(f\"\\n  è®­ç»ƒæŸå¤±: {avg_train_loss:.4f}\")\n",
    "        print(f\"  éªŒè¯ MAE: {val_mae:.4f}\")\n",
    "        print(f\"  ç”¨æ—¶: {time.time() - epoch_start_time:.1f}s\")\n",
    "        \n",
    "        # æ¸…ç†éªŒè¯æ•°æ®\n",
    "        data_loader.clear_data()\n",
    "        \n",
    "        # æ›´æ–°å­¦ä¹ ç‡\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # ä¿å­˜æœ€ä½³æ¨¡å‹\n",
    "        if val_mae < min_val_loss:\n",
    "            min_val_loss = val_mae\n",
    "            best_epoch = epoch\n",
    "            torch.save(model.state_dict(), args['model_file'])\n",
    "            log_string(log, f'\\nâœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Epoch {epoch}, MAE={val_mae:.4f})')\n",
    "            print(f\"âœ… ä¿å­˜æœ€ä½³æ¨¡å‹ (Epoch {epoch}, MAE={val_mae:.4f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ‰ è®­ç»ƒå®Œæˆï¼\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"æœ€ä½³ Epoch: {best_epoch}\")\n",
    "    print(f\"æœ€ä½³éªŒè¯ MAE: {min_val_loss:.4f}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ è®­ç»ƒå‡ºé”™: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab96cc99",
   "metadata": {},
   "source": [
    "## 9. æµ‹è¯•é›†è¯„ä¼°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c890516",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=\" * 80)\n",
    "print(\"ğŸ“Š æµ‹è¯•é›†è¯„ä¼°\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# åŠ è½½æœ€ä½³æ¨¡å‹\n",
    "model.load_state_dict(torch.load(args['model_file'], map_location=device))\n",
    "print(f\"âœ… åŠ è½½æœ€ä½³æ¨¡å‹: {args['model_file']}\\n\")\n",
    "\n",
    "# åŠ è½½æµ‹è¯•æ•°æ®\n",
    "data_loader.load_data_for_date_range(date_batches[-1][0], date_batches[-1][1])\n",
    "testX, testY, testXTE, testYTE = data_loader.get_test_data()\n",
    "\n",
    "model.eval()\n",
    "num_test = testX.shape[0]\n",
    "pred = []\n",
    "label = []\n",
    "\n",
    "num_batch = math.ceil(num_test / args['batch_size'])\n",
    "\n",
    "print(\"é¢„æµ‹ä¸­...\")\n",
    "with torch.no_grad():\n",
    "    for batch_idx in tqdm(range(num_batch), desc=\"æµ‹è¯•\"):\n",
    "        start_idx = batch_idx * args['batch_size']\n",
    "        end_idx = min(num_test, (batch_idx + 1) * args['batch_size'])\n",
    "\n",
    "        X = testX[start_idx:end_idx]\n",
    "        Y = testY[start_idx:end_idx]\n",
    "        TE = torch.from_numpy(testXTE[start_idx:end_idx]).to(device)\n",
    "        NormX = torch.from_numpy((X - mean) / std).float().to(device)\n",
    "\n",
    "        y_hat = model(NormX, TE)\n",
    "        pred.append(y_hat.cpu().numpy() * std + mean)\n",
    "        label.append(Y)\n",
    "\n",
    "pred = np.concatenate(pred, axis=0)\n",
    "label = np.concatenate(label, axis=0)\n",
    "\n",
    "mae, rmse, mape = metric(pred, label)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“ˆ æœ€ç»ˆæµ‹è¯•ç»“æœ\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"  MAE:  {mae:.4f}\")\n",
    "print(f\"  RMSE: {rmse:.4f}\")\n",
    "print(f\"  MAPE: {mape:.4f}\")\n",
    "print(\"=\" * 80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fb1421",
   "metadata": {},
   "source": [
    "## 10. å…³é—­æ—¥å¿—"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c14e382",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_string(log, f'\\nç»“æŸæ—¶é—´: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')\n",
    "log.close()\n",
    "\n",
    "print(f\"\\nâœ… æ—¥å¿—å·²ä¿å­˜: {args['log_file']}\")\n",
    "print(f\"âœ… æ¨¡å‹å·²ä¿å­˜: {args['model_file']}\")\n",
    "print(\"\\nğŸ‰ å…¨éƒ¨å®Œæˆï¼\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
